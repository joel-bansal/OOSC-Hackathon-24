{
    "links": [
        "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd740493ff328&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&source=---two_column_layout_nav----------------------------------",
        "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9045b85edb78&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40atulkumar_68871%2Fincorporating-chat-history-in-conversational-agents-using-langchain-9045b85edb78&source=-----d740493ff328----3-----------------bookmark_preview----37a57b5c_a8c1_4311_9649_c27449e17477-------",
        "https://medium.com/search?source=---two_column_layout_nav----------------------------------",
        "https://medium.com/@eric_vaillancourt/mastering-langchain-rag-integrating-chat-history-part-2-4c80eae11b43?source=read_next_recirc-----d740493ff328----1---------------------37a57b5c_a8c1_4311_9649_c27449e17477-------",
        "https://medium.com/tag/langchain?source=post_page-----d740493ff328---------------langchain-----------------",
        "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fef3dea01afbc&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chabots-semantic-router-user-intents-ef3dea01afbc&source=-----d740493ff328----1-----------------bookmark_preview----63ab3ec8_fc15_4d66_ac2b_30d7b869f768-------",
        "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d740493ff328--------------------------------",
        "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F0189a18ec401&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40suraj_bansal%2Fbuild-your-own-ai-chatbot-a-beginners-guide-to-rag-and-langchain-0189a18ec401&source=-----d740493ff328----2-----------------bookmark_preview----37a57b5c_a8c1_4311_9649_c27449e17477-------",
        "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fd740493ff328&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&user=Tal+Waitzenberg&userId=55ec5990cf02&source=-----d740493ff328---------------------clap_footer-----------",
        "https://vijaykumarkartha.medium.com/beginners-guide-to-conversational-retrieval-chain-using-langchain-3ddf1357f371?source=read_next_recirc-----d740493ff328----0---------------------37a57b5c_a8c1_4311_9649_c27449e17477-------",
        "https://medium.com/@atulkumar_68871?source=read_next_recirc-----d740493ff328----3---------------------37a57b5c_a8c1_4311_9649_c27449e17477-------",
        "https://tomsmith585.medium.com/list/generative-ai-recommended-reading-508b0743c247?source=read_next_recirc-----d740493ff328--------------------------------",
        "https://vijaykumarkartha.medium.com/?source=read_next_recirc-----d740493ff328----0---------------------37a57b5c_a8c1_4311_9649_c27449e17477-------",
        "https://medium.com/tag/conversational-ai?source=post_page-----d740493ff328---------------conversational_ai-----------------",
        "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&source=post_page---two_column_layout_nav-----------------------global_nav-----------",
        "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2Fplans%3Fdimension%3Dpost_audio_button%26postId%3Dd740493ff328&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&source=-----d740493ff328---------------------post_audio_button-----------",
        "https://medium.com/@nicholas.michael.janulewicz/list/chatgpt-prompts-b4c47b8e12ee?source=read_next_recirc-----d740493ff328--------------------------------",
        "https://medium.statuspage.io/?source=post_page-----d740493ff328--------------------------------",
        "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd740493ff328&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&source=-----d740493ff328---------------------bookmark_footer-----------",
        "https://medium.com/@atulkumar_68871/incorporating-chat-history-in-conversational-agents-using-langchain-9045b85edb78?source=read_next_recirc-----d740493ff328----3---------------------37a57b5c_a8c1_4311_9649_c27449e17477-------",
        "https://medium.com/tag/llm?source=post_page-----d740493ff328---------------llm-----------------",
        "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4c80eae11b43&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40eric_vaillancourt%2Fmastering-langchain-rag-integrating-chat-history-part-2-4c80eae11b43&source=-----d740493ff328----1-----------------bookmark_preview----37a57b5c_a8c1_4311_9649_c27449e17477-------",
        "https://speechify.com/medium?source=post_page-----d740493ff328--------------------------------",
        "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F0773cf4e70ad&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-semantic-router-rag-gateway-part-1-0773cf4e70ad&source=-----d740493ff328----0-----------------bookmark_preview----63ab3ec8_fc15_4d66_ac2b_30d7b869f768-------",
        "https://medium.com/@suraj_bansal/build-your-own-ai-chatbot-a-beginners-guide-to-rag-and-langchain-0189a18ec401?source=read_next_recirc-----d740493ff328----2---------------------37a57b5c_a8c1_4311_9649_c27449e17477-------",
        "https://medium.com/business?source=post_page-----d740493ff328--------------------------------",
        "https://medium.com/?source=post_page-----d740493ff328--------------------------------",
        "https://lmy.medium.com/?source=read_next_recirc-----d740493ff328----0---------------------37a57b5c_a8c1_4311_9649_c27449e17477-------",
        "https://medium.com/@suraj_bansal?source=read_next_recirc-----d740493ff328----2---------------------37a57b5c_a8c1_4311_9649_c27449e17477-------",
        "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&source=---post_footer_upsell--d740493ff328---------------------lo_non_moc_upsell-----------",
        "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F793fb9c3580c&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40talon8080%2Fmastering-rag-chabots-the-road-from-a-poc-to-production-793fb9c3580c&source=-----d740493ff328----2-----------------bookmark_preview----63ab3ec8_fc15_4d66_ac2b_30d7b869f768-------",
        "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F55ec5990cf02&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&user=Tal+Waitzenberg&userId=55ec5990cf02&source=post_page-55ec5990cf02----d740493ff328---------------------follow_profile-----------",
        "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc3c8261f66b3&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&newsletterV3=55ec5990cf02&newsletterV3Id=c3c8261f66b3&user=Tal+Waitzenberg&userId=55ec5990cf02&source=-----d740493ff328---------------------subscribe_user-----------",
        "https://medium.com/tag/genai?source=post_page-----d740493ff328---------------genai-----------------",
        "https://medium.com/@AMGAS14/list/natural-language-processing-0a856388a93a?source=read_next_recirc-----d740493ff328--------------------------------",
        "https://medium.com/@MediumStaff/list/staff-picks-c7bc6e1ee00f?source=read_next_recirc-----d740493ff328--------------------------------",
        "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2970140edf33&operation=register&redirect=https%3A%2F%2Flmy.medium.com%2Fcomparing-langchain-and-llamaindex-with-4-tasks-2970140edf33&source=-----d740493ff328----0-----------------bookmark_preview----37a57b5c_a8c1_4311_9649_c27449e17477-------",
        "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3ddf1357f371&operation=register&redirect=https%3A%2F%2Fvijaykumarkartha.medium.com%2Fbeginners-guide-to-conversational-retrieval-chain-using-langchain-3ddf1357f371&source=-----d740493ff328----0-----------------bookmark_preview----37a57b5c_a8c1_4311_9649_c27449e17477-------",
        "https://medium.com/@eric_vaillancourt?source=read_next_recirc-----d740493ff328----1---------------------37a57b5c_a8c1_4311_9649_c27449e17477-------",
        "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F793fb9c3580c&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40talon8080%2Fmastering-rag-chabots-the-road-from-a-poc-to-production-793fb9c3580c&source=-----d740493ff328----1-----------------bookmark_preview----37a57b5c_a8c1_4311_9649_c27449e17477-------",
        "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd740493ff328&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&source=--------------------------bookmark_footer-----------",
        "https://help.medium.com/hc/en-us?source=post_page-----d740493ff328--------------------------------",
        "https://blog.medium.com/?source=post_page-----d740493ff328--------------------------------",
        "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----d740493ff328--------------------------------",
        "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fplans&source=---post_footer_upsell--d740493ff328---------------------lo_non_moc_upsell-----------",
        "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------",
        "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d740493ff328--------------------------------",
        "https://medium.com/?source=---two_column_layout_nav----------------------------------",
        "https://medium.com/about?autoplay=1&source=post_page-----d740493ff328--------------------------------",
        "https://medium.com/tag/retrieval-augmented?source=post_page-----d740493ff328---------------retrieval_augmented-----------------",
        "https://lmy.medium.com/comparing-langchain-and-llamaindex-with-4-tasks-2970140edf33?source=read_next_recirc-----d740493ff328----0---------------------37a57b5c_a8c1_4311_9649_c27449e17477-------",
        "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F55ec5990cf02&operation=register&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&user=Tal+Waitzenberg&userId=55ec5990cf02&source=post_page-55ec5990cf02----d740493ff328---------------------post_header-----------",
        "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftalwaitzenberg.com%2Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328&source=post_page---two_column_layout_nav-----------------------global_nav-----------"
    ],
    "data": "html: Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChain | by Tal Waitzenberg | MediumOpen in appSign upSign inWriteSign upSign inMastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShareIn the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.Designing the Conversation FlowConversational RAG FlowThe conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.Crafting Prompts with the COSTAR FrameworkTo generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.The COSTAR prompt structure is:# Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM]Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.Implementing Our Conversational Flow as a Chain in LangChainTo begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain.@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod.Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework:REPHARSING_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #STANDALONE_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question #ROUTER_DECISION_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass:classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}Final WordsThis blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications.Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/monthGenaiLangchainConversational AIRetrieval AugmentedLlm185185FollowWritten byTal Waitzenberg69 FollowersI am a pioneer in Generative AI and Data Science, blending AI and ML to craft cross-domain solutions. My expertise - leveraging Classical ML, AI and LLMs..FollowMore from Tal WaitzenbergTal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285See all from Tal WaitzenbergRecommended from MediumMingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285ListsNatural Language Processing1662 stories·1237 savesChatGPT prompts48 stories·1930 savesGenerative AI Recommended Reading52 stories·1302 savesStaff Picks718 stories·1248 savesVijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams head: Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChain | by Tal Waitzenberg | Medium title: Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChain | by Tal Waitzenberg | Medium script: {\"@context\":\"http:\\u002F\\u002Fschema.org\",\"@type\":\"NewsArticle\",\"image\":[\"https:\\u002F\\u002Fmiro.medium.com\\u002Fv2\\u002Fresize:fit:1200\\u002F1*Ok5uPTnHHr8WB09oNHl1tQ.png\"],\"url\":\"https:\\u002F\\u002Ftalwaitzenberg.com\\u002Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328\",\"dateCreated\":\"2024-03-11T22:14:53.786Z\",\"datePublished\":\"2024-03-11T22:14:53.786Z\",\"dateModified\":\"2024-03-11T22:28:51.054Z\",\"headline\":\"Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChain\",\"name\":\"Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChain\",\"description\":\"In the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However…\",\"identifier\":\"d740493ff328\",\"author\":{\"@type\":\"Person\",\"name\":\"Tal Waitzenberg\",\"url\":\"https:\\u002F\\u002Ftalwaitzenberg.com\"},\"creator\":[\"Tal Waitzenberg\"],\"publisher\":{\"@type\":\"Organization\",\"name\":\"Medium\",\"url\":\"https:\\u002F\\u002Ftalwaitzenberg.com\\u002F\",\"logo\":{\"@type\":\"ImageObject\",\"width\":272,\"height\":60,\"url\":\"https:\\u002F\\u002Fmiro.medium.com\\u002Fv2\\u002Fresize:fit:544\\u002F7*V1_7XP4snlmqrc_0Njontw.png\"}},\"mainEntityOfPage\":\"https:\\u002F\\u002Ftalwaitzenberg.com\\u002Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328\"} style: html{box-sizing:border-box;-webkit-text-size-adjust:100%}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}:root{--reach-tabs:1;--reach-menu-button:1}#speechify-root{font-family:Sohne, sans-serif}div[data-popper-reference-hidden=\"true\"]{visibility:hidden;pointer-events:none}.grecaptcha-badge{visibility:hidden}\n/*XCode style (c) Angel Garcia <angelgarcia.mail@gmail.com>*/.hljs {background: #fff;color: black;\n}/* Gray DOCTYPE selectors like WebKit */\n.xml .hljs-meta {color: #c0c0c0;\n}.hljs-comment,\n.hljs-quote {color: #007400;\n}.hljs-tag,\n.hljs-attribute,\n.hljs-keyword,\n.hljs-selector-tag,\n.hljs-literal,\n.hljs-name {color: #aa0d91;\n}.hljs-variable,\n.hljs-template-variable {color: #3F6E74;\n}.hljs-code,\n.hljs-string,\n.hljs-meta .hljs-string {color: #c41a16;\n}.hljs-regexp,\n.hljs-link {color: #0E0EFF;\n}.hljs-title,\n.hljs-symbol,\n.hljs-bullet,\n.hljs-number {color: #1c00cf;\n}.hljs-section,\n.hljs-meta {color: #643820;\n}.hljs-title.class_,\n.hljs-class .hljs-title,\n.hljs-type,\n.hljs-built_in,\n.hljs-params {color: #5c2699;\n}.hljs-attr {color: #836C28;\n}.hljs-subst {color: #000;\n}.hljs-formula {background-color: #eee;font-style: italic;\n}.hljs-addition {background-color: #baeeba;\n}.hljs-deletion {background-color: #ffc8bd;\n}.hljs-selector-id,\n.hljs-selector-class {color: #9b703f;\n}.hljs-doctag,\n.hljs-strong {font-weight: bold;\n}.hljs-emphasis {font-style: italic;\n} style: @-webkit-keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@-moz-keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}} style: .a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, Oxygen, Ubuntu, Cantarell, \"Open Sans\", \"Helvetica Neue\", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{display:block}.m{position:sticky}.n{top:0}.o{z-index:500}.p{padding:0 24px}.q{align-items:center}.r{border-bottom:solid 1px #F2F2F2}.y{height:41px}.z{line-height:20px}.ab{display:flex}.ac{height:57px}.ae{flex:1 0 auto}.af{color:inherit}.ag{fill:inherit}.ah{font-size:inherit}.ai{border:inherit}.aj{font-family:inherit}.ak{letter-spacing:inherit}.al{font-weight:inherit}.am{padding:0}.an{margin:0}.ao{cursor:pointer}.ap:disabled{cursor:not-allowed}.aq:disabled{color:#6B6B6B}.ar:disabled{fill:#6B6B6B}.au{width:auto}.av path{fill:#242424}.aw{height:25px}.ax{margin-left:16px}.ay{border:none}.az{border-radius:20px}.ba{width:240px}.bb{background:#F9F9F9}.bc path{fill:#6B6B6B}.be{outline:none}.bf{font-family:sohne, \"Helvetica Neue\", Helvetica, Arial, sans-serif}.bg{font-size:14px}.bh{width:100%}.bi{padding:10px 20px 10px 0}.bj{background-color:transparent}.bk{color:#242424}.bl::placeholder{color:#6B6B6B}.bm{display:inline-block}.bn{margin-left:12px}.bo{margin-right:12px}.bp{border-radius:4px}.bq{margin-left:24px}.br{height:24px}.bx{background-color:#F9F9F9}.by{border-radius:50%}.bz{height:32px}.ca{width:32px}.cb{justify-content:center}.ch{max-width:680px}.ci{min-width:0}.cj{animation:k1 1.2s ease-in-out infinite}.ck{height:100vh}.cl{margin-bottom:16px}.cm{margin-top:48px}.cn{align-items:flex-start}.co{flex-direction:column}.cp{justify-content:space-between}.cq{margin-bottom:24px}.cw{width:80%}.cx{background-color:#F2F2F2}.dd{height:44px}.de{width:44px}.df{margin:auto 0}.dg{margin-bottom:4px}.dh{height:16px}.di{width:120px}.dj{width:80px}.dp{margin-bottom:8px}.dq{width:96%}.dr{width:98%}.ds{width:81%}.dt{margin-left:8px}.du{color:#6B6B6B}.dv{font-size:13px}.dw{height:100%}.ep{color:#FFFFFF}.eq{fill:#FFFFFF}.er{background:#1A8917}.es{border-color:#1A8917}.ew:disabled{cursor:inherit !important}.ex:disabled{opacity:0.3}.ey:disabled:hover{background:#1A8917}.ez:disabled:hover{border-color:#1A8917}.fa{border-radius:99em}.fb{border-width:1px}.fc{border-style:solid}.fd{box-sizing:border-box}.fe{text-decoration:none}.ff{text-align:center}.fi{margin-right:32px}.fj{position:relative}.fk{fill:#6B6B6B}.fn{background:transparent}.fo svg{margin-left:4px}.fp svg{fill:#6B6B6B}.fr{box-shadow:inset 0 0 0 1px rgba(0, 0, 0, 0.05)}.fs{position:absolute}.fz{margin:0 24px}.gd{background:rgba(255, 255, 255, 1)}.ge{border:1px solid #F2F2F2}.gf{box-shadow:0 1px 4px #F2F2F2}.gg{max-height:100vh}.gh{overflow-y:auto}.gi{left:0}.gj{top:calc(100vh + 100px)}.gk{bottom:calc(100vh + 100px)}.gl{width:10px}.gm{pointer-events:none}.gn{word-break:break-word}.go{word-wrap:break-word}.gp:after{display:block}.gq:after{content:\"\"}.gr:after{clear:both}.gs{line-height:1.23}.gt{letter-spacing:0}.gu{font-style:normal}.gv{font-weight:700}.ia{align-items:baseline}.ib{width:48px}.ic{height:48px}.id{border:2px solid rgba(255, 255, 255, 1)}.ie{z-index:0}.if{box-shadow:none}.ig{border:1px solid rgba(0, 0, 0, 0.05)}.ih{margin-bottom:2px}.ii{flex-wrap:nowrap}.ij{font-size:16px}.ik{line-height:24px}.im{margin:0 8px}.in{display:inline}.io{color:#1A8917}.ip{fill:#1A8917}.is{flex:0 0 auto}.iv{flex-wrap:wrap}.iw{padding-left:8px}.ix{padding-right:8px}.jy> *{flex-shrink:0}.jz{overflow-x:scroll}.ka::-webkit-scrollbar{display:none}.kb{scrollbar-width:none}.kc{-ms-overflow-style:none}.kd{width:74px}.ke{flex-direction:row}.kf{z-index:2}.kg{margin-right:4px}.kj{-webkit-user-select:none}.kk{border:0}.kl{fill:rgba(117, 117, 117, 1)}.ko{outline:0}.kp{user-select:none}.kq> svg{pointer-events:none}.kz{cursor:progress}.la{opacity:1}.lb{padding:4px 0}.le{margin-top:0px}.lf{width:16px}.lh{display:inline-flex}.ln{max-width:100%}.lo{padding:8px 2px}.lp svg{color:#6B6B6B}.mg{margin-left:auto}.mh{margin-right:auto}.mi{max-width:858px}.mo{clear:both}.mq{cursor:zoom-in}.mr{z-index:auto}.mt{height:auto}.mu{line-height:1.58}.mv{letter-spacing:-0.004em}.mw{font-family:source-serif-pro, Georgia, Cambria, \"Times New Roman\", Times, serif}.nr{margin-bottom:-0.46em}.ns{line-height:1.18}.nt{letter-spacing:-0.022em}.nu{font-weight:600}.ok{margin-bottom:-0.31em}.ol{max-width:1078px}.or{margin-top:10px}.os{max-width:728px}.ov{list-style-type:decimal}.ow{margin-left:30px}.ox{padding-left:0px}.pd{list-style-type:disc}.pe{font-style:italic}.pk{overflow-x:auto}.pl{font-family:source-code-pro, Menlo, Monaco, \"Courier New\", Courier, monospace}.pm{padding:32px}.pn{border:1px solid #E5E5E5}.po{line-height:1.4}.pp{margin-top:-0.2em}.pq{margin-bottom:-0.2em}.pr{white-space:pre}.ps{min-width:fit-content}.pt{padding:2px 4px}.pu{font-size:75%}.pv> strong{font-family:inherit}.pw{margin-bottom:26px}.px{margin-top:6px}.py{margin-top:8px}.pz{margin-right:8px}.qa{padding:8px 16px}.qb{border-radius:100px}.qc{transition:background 300ms ease}.qe{white-space:nowrap}.qf{border-top:none}.ql{height:52px}.qm{max-height:52px}.qn{box-sizing:content-box}.qo{position:static}.qp{z-index:1}.qr{max-width:155px}.qx{margin-right:20px}.rd{align-items:flex-end}.re{width:76px}.rf{height:76px}.rg{border:2px solid #F9F9F9}.rh{height:72px}.ri{width:72px}.rj{stroke:#F2F2F2}.rk{height:36px}.rl{width:36px}.rm{color:#F2F2F2}.rn{fill:#F2F2F2}.ro{background:#F2F2F2}.rp{border-color:#F2F2F2}.rv{font-weight:500}.rw{font-size:24px}.rx{line-height:30px}.ry{letter-spacing:-0.016em}.rz{margin-top:16px}.sa{height:0px}.sb{border-bottom:solid 1px #E5E5E5}.sc{margin-top:72px}.sd{padding:24px 0}.se{margin-bottom:0px}.sf{margin-right:16px}.as:hover:not(:disabled){color:rgba(25, 25, 25, 1)}.at:hover:not(:disabled){fill:rgba(25, 25, 25, 1)}.et:hover{background:#156D12}.eu:hover{border-color:#156D12}.ev:hover{cursor:pointer}.fl:hover{color:#242424}.fm:hover{fill:#242424}.fq:hover svg{fill:#242424}.ft:hover{background-color:rgba(0, 0, 0, 0.1)}.il:hover{text-decoration:underline}.iq:hover:not(:disabled){color:#156D12}.ir:hover:not(:disabled){fill:#156D12}.kn:hover{fill:rgba(8, 8, 8, 1)}.lc:hover{fill:#000000}.ld:hover p{color:#000000}.lg:hover{color:#000000}.lq:hover svg{color:#000000}.qd:hover{background-color:#F2F2F2}.rq:hover{background:#F2F2F2}.rr:hover{border-color:#F2F2F2}.rs:hover{cursor:wait}.rt:hover{color:#F2F2F2}.ru:hover{fill:#F2F2F2}.bd:focus-within path{fill:#242424}.km:focus{fill:rgba(8, 8, 8, 1)}.lr:focus svg{color:#000000}.ms:focus{transform:scale(1.01)}.kr:active{border-style:none} style: .d{display:none}.bw{width:64px}.cg{margin:0 64px}.cv{height:48px}.dc{margin-bottom:52px}.do{margin-bottom:48px}.ef{font-size:14px}.eg{line-height:20px}.em{font-size:13px}.eo{padding:5px 12px}.fh{display:flex}.fy{margin-bottom:68px}.gc{max-width:680px}.hq{font-size:42px}.hr{margin-top:1.19em}.hs{margin-bottom:32px}.ht{line-height:52px}.hu{letter-spacing:-0.011em}.hz{align-items:center}.jk{border-top:solid 1px #F2F2F2}.jl{border-bottom:solid 1px #F2F2F2}.jm{margin:32px 0 0}.jn{padding:3px 8px}.jw> *{margin-right:24px}.jx> :last-child{margin-right:0}.ky{margin-top:0px}.lm{margin:0}.mn{margin-top:40px}.nn{font-size:20px}.no{margin-top:2.14em}.np{line-height:32px}.nq{letter-spacing:-0.003em}.oh{margin-top:1.72em}.oi{line-height:24px}.oj{letter-spacing:0}.oq{margin-top:56px}.pc{margin-top:1.14em}.pj{margin-top:0.94em}.qk{margin-bottom:88px}.qw{display:inline-block}.rc{padding-top:72px} style: .e{display:none}.kx{margin-top:0px}.ot{margin-left:auto}.ou{text-align:center}.qv{display:inline-block} style: .f{display:none}.kw{margin-top:0px}.qu{display:inline-block} style: .g{display:none}.ku{margin-top:0px}.kv{margin-right:0px}.qt{display:inline-block} style: .h{display:none}.s{display:flex}.t{justify-content:space-between}.bs{width:24px}.cc{margin:0 24px}.cr{height:40px}.cy{margin-bottom:44px}.dk{margin-bottom:32px}.dx{font-size:13px}.dy{line-height:20px}.eh{padding:0px 8px 1px}.fu{margin-bottom:4px}.gw{font-size:32px}.gx{margin-top:1.01em}.gy{margin-bottom:24px}.gz{line-height:38px}.ha{letter-spacing:-0.014em}.hv{align-items:flex-start}.it{flex-direction:column}.iy{margin:24px -24px 0}.iz{padding:0}.jo> *{margin-right:8px}.jp> :last-child{margin-right:24px}.kh{margin-left:0px}.ks{margin-top:0px}.kt{margin-right:0px}.li{margin:0}.ls{border:1px solid #F2F2F2}.lt{border-radius:99em}.lu{padding:0px 16px 0px 12px}.lv{height:38px}.lw{align-items:center}.ly svg{margin-right:8px}.mj{margin-top:32px}.mx{font-size:18px}.my{margin-top:1.56em}.mz{line-height:28px}.na{letter-spacing:-0.003em}.nv{font-size:16px}.nw{margin-top:1.23em}.nx{letter-spacing:0}.om{margin-top:40px}.oy{margin-top:1.34em}.pf{margin-top:0.67em}.qg{margin-bottom:80px}.qs{display:inline-block}.qy{padding-top:48px}.lx:hover{border-color:#E5E5E5} style: .i{display:none}.bv{width:64px}.cf{margin:0 64px}.cu{height:48px}.db{margin-bottom:52px}.dn{margin-bottom:48px}.ed{font-size:14px}.ee{line-height:20px}.ek{font-size:13px}.el{padding:5px 12px}.fg{display:flex}.fx{margin-bottom:68px}.gb{max-width:680px}.hl{font-size:42px}.hm{margin-top:1.19em}.hn{margin-bottom:32px}.ho{line-height:52px}.hp{letter-spacing:-0.011em}.hy{align-items:center}.jg{border-top:solid 1px #F2F2F2}.jh{border-bottom:solid 1px #F2F2F2}.ji{margin:32px 0 0}.jj{padding:3px 8px}.ju> *{margin-right:24px}.jv> :last-child{margin-right:0}.ll{margin:0}.mm{margin-top:40px}.nj{font-size:20px}.nk{margin-top:2.14em}.nl{line-height:32px}.nm{letter-spacing:-0.003em}.oe{margin-top:1.72em}.of{line-height:24px}.og{letter-spacing:0}.op{margin-top:56px}.pb{margin-top:1.14em}.pi{margin-top:0.94em}.qj{margin-bottom:88px}.rb{padding-top:72px} style: .j{display:none}.w{display:flex}.x{justify-content:space-between}.bu{width:64px}.ce{margin:0 48px}.ct{height:48px}.da{margin-bottom:52px}.dm{margin-bottom:48px}.eb{font-size:13px}.ec{line-height:20px}.ej{padding:0px 8px 1px}.fw{margin-bottom:68px}.ga{max-width:680px}.hg{font-size:42px}.hh{margin-top:1.19em}.hi{margin-bottom:32px}.hj{line-height:52px}.hk{letter-spacing:-0.011em}.hx{align-items:center}.jc{border-top:solid 1px #F2F2F2}.jd{border-bottom:solid 1px #F2F2F2}.je{margin:32px 0 0}.jf{padding:3px 8px}.js> *{margin-right:24px}.jt> :last-child{margin-right:0}.lk{margin:0}.ml{margin-top:40px}.nf{font-size:20px}.ng{margin-top:2.14em}.nh{line-height:32px}.ni{letter-spacing:-0.003em}.ob{margin-top:1.72em}.oc{line-height:24px}.od{letter-spacing:0}.oo{margin-top:56px}.pa{margin-top:1.14em}.ph{margin-top:0.94em}.qi{margin-bottom:88px}.ra{padding-top:72px} style: .k{display:none}.u{display:flex}.v{justify-content:space-between}.bt{width:24px}.cd{margin:0 24px}.cs{height:40px}.cz{margin-bottom:44px}.dl{margin-bottom:32px}.dz{font-size:13px}.ea{line-height:20px}.ei{padding:0px 8px 1px}.fv{margin-bottom:4px}.hb{font-size:32px}.hc{margin-top:1.01em}.hd{margin-bottom:24px}.he{line-height:38px}.hf{letter-spacing:-0.014em}.hw{align-items:flex-start}.iu{flex-direction:column}.ja{margin:24px 0 0}.jb{padding:0}.jq> *{margin-right:8px}.jr> :last-child{margin-right:8px}.ki{margin-left:0px}.lj{margin:0}.lz{border:1px solid #F2F2F2}.ma{border-radius:99em}.mb{padding:0px 16px 0px 12px}.mc{height:38px}.md{align-items:center}.mf svg{margin-right:8px}.mk{margin-top:32px}.nb{font-size:18px}.nc{margin-top:1.56em}.nd{line-height:28px}.ne{letter-spacing:-0.003em}.ny{font-size:16px}.nz{margin-top:1.23em}.oa{letter-spacing:0}.on{margin-top:40px}.oz{margin-top:1.34em}.pg{margin-top:0.67em}.qh{margin-bottom:80px}.qz{padding-top:48px}.me:hover{border-color:#E5E5E5} style: .qq{display:none} style: .mp{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)} script: window.dataLayer = window.dataLayer || [];\n            function gtag(){dataLayer.push(arguments);}\n            gtag('js', new Date());\n\n            gtag('config', 'G-7JY7T788PK'); script: (function(b,r,a,n,c,h,_,s,d,k){if(!b[n]||!b[n]._q){for(;s<_.length;)c(h,_[s++]);d=r.createElement(a);d.async=1;d.src=\"https://cdn.branch.io/branch-latest.min.js\";k=r.getElementsByTagName(a)[0];k.parentNode.insertBefore(d,k);b[n]=h}})(window,document,\"script\",\"branch\",function(b,r){b[r]=function(){b._q.push([r,arguments])}},{_q:[],_v:1},\"addListener applyCode autoAppIndex banner closeBanner closeJourney creditHistory credits data deepview deepviewCta first getCode init link logout redeem referrals removeListener sendSMS setBranchViewData setIdentity track validateCode trackCommerceEvent logEvent\".split(\" \"), 0);\nbranch.init('key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm', {metadata: {}, 'no_journeys': true, 'disable_exit_animation': true, 'disable_entry_animation': true, 'tracking_disabled': null}, function(err, data) {}); body: Open in appSign upSign inWriteSign upSign inMastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShareIn the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.Designing the Conversation FlowConversational RAG FlowThe conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.Crafting Prompts with the COSTAR FrameworkTo generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.The COSTAR prompt structure is:# Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM]Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.Implementing Our Conversational Flow as a Chain in LangChainTo begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain.@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod.Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework:REPHARSING_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #STANDALONE_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question #ROUTER_DECISION_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass:classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}Final WordsThis blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications.Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/monthGenaiLangchainConversational AIRetrieval AugmentedLlm185185FollowWritten byTal Waitzenberg69 FollowersI am a pioneer in Generative AI and Data Science, blending AI and ML to craft cross-domain solutions. My expertise - leveraging Classical ML, AI and LLMs..FollowMore from Tal WaitzenbergTal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285See all from Tal WaitzenbergRecommended from MediumMingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285ListsNatural Language Processing1662 stories·1237 savesChatGPT prompts48 stories·1930 savesGenerative AI Recommended Reading52 stories·1302 savesStaff Picks718 stories·1248 savesVijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams div: Open in appSign upSign inWriteSign upSign inMastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShareIn the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.Designing the Conversation FlowConversational RAG FlowThe conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.Crafting Prompts with the COSTAR FrameworkTo generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.The COSTAR prompt structure is:# Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM]Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.Implementing Our Conversational Flow as a Chain in LangChainTo begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain.@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod.Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework:REPHARSING_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #STANDALONE_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question #ROUTER_DECISION_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass:classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}Final WordsThis blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications.Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/monthGenaiLangchainConversational AIRetrieval AugmentedLlm185185FollowWritten byTal Waitzenberg69 FollowersI am a pioneer in Generative AI and Data Science, blending AI and ML to craft cross-domain solutions. My expertise - leveraging Classical ML, AI and LLMs..FollowMore from Tal WaitzenbergTal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285See all from Tal WaitzenbergRecommended from MediumMingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285ListsNatural Language Processing1662 stories·1237 savesChatGPT prompts48 stories·1930 savesGenerative AI Recommended Reading52 stories·1302 savesStaff Picks718 stories·1248 savesVijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams div: Open in appSign upSign inWriteSign upSign inMastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShareIn the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.Designing the Conversation FlowConversational RAG FlowThe conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.Crafting Prompts with the COSTAR FrameworkTo generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.The COSTAR prompt structure is:# Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM]Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.Implementing Our Conversational Flow as a Chain in LangChainTo begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain.@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod.Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework:REPHARSING_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #STANDALONE_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question #ROUTER_DECISION_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass:classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}Final WordsThis blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications.Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/monthGenaiLangchainConversational AIRetrieval AugmentedLlm185185FollowWritten byTal Waitzenberg69 FollowersI am a pioneer in Generative AI and Data Science, blending AI and ML to craft cross-domain solutions. My expertise - leveraging Classical ML, AI and LLMs..FollowMore from Tal WaitzenbergTal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285See all from Tal WaitzenbergRecommended from MediumMingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285ListsNatural Language Processing1662 stories·1237 savesChatGPT prompts48 stories·1930 savesGenerative AI Recommended Reading52 stories·1302 savesStaff Picks718 stories·1248 savesVijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams script: document.domain = document.domain; script: if (window.self !== window.top) window.location = \"about:blank\" div: Open in appSign upSign inWriteSign upSign inMastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShareIn the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.Designing the Conversation FlowConversational RAG FlowThe conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.Crafting Prompts with the COSTAR FrameworkTo generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.The COSTAR prompt structure is:# Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM]Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.Implementing Our Conversational Flow as a Chain in LangChainTo begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain.@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod.Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework:REPHARSING_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #STANDALONE_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question #ROUTER_DECISION_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass:classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}Final WordsThis blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications.Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/monthGenaiLangchainConversational AIRetrieval AugmentedLlm185185FollowWritten byTal Waitzenberg69 FollowersI am a pioneer in Generative AI and Data Science, blending AI and ML to craft cross-domain solutions. My expertise - leveraging Classical ML, AI and LLMs..FollowMore from Tal WaitzenbergTal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285See all from Tal WaitzenbergRecommended from MediumMingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285ListsNatural Language Processing1662 stories·1237 savesChatGPT prompts48 stories·1930 savesGenerative AI Recommended Reading52 stories·1302 savesStaff Picks718 stories·1248 savesVijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams div: Open in appSign upSign inWriteSign upSign in div: Open in appSign upSign in a: Open in app div: Sign upSign in p: Sign up span: Sign up a: Sign up div: Sign in p: Sign in span: Sign in a: Sign in div: WriteSign upSign in div: Write div: Write span: Write a: Write div: Write div: Write div: Sign upSign in div: Sign upSign in p: Sign up span: Sign up a: Sign up div: Sign in p: Sign in span: Sign in a: Sign in div: Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShareIn the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.Designing the Conversation FlowConversational RAG FlowThe conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.Crafting Prompts with the COSTAR FrameworkTo generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.The COSTAR prompt structure is:# Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM]Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.Implementing Our Conversational Flow as a Chain in LangChainTo begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain.@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod.Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework:REPHARSING_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #STANDALONE_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question #ROUTER_DECISION_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass:classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}Final WordsThis blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications.Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/monthGenaiLangchainConversational AIRetrieval AugmentedLlm185185FollowWritten byTal Waitzenberg69 FollowersI am a pioneer in Generative AI and Data Science, blending AI and ML to craft cross-domain solutions. My expertise - leveraging Classical ML, AI and LLMs..FollowMore from Tal WaitzenbergTal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285See all from Tal WaitzenbergRecommended from MediumMingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285ListsNatural Language Processing1662 stories·1237 savesChatGPT prompts48 stories·1930 savesGenerative AI Recommended Reading52 stories·1302 savesStaff Picks718 stories·1248 savesVijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams div: Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShareIn the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.Designing the Conversation FlowConversational RAG FlowThe conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.Crafting Prompts with the COSTAR FrameworkTo generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.The COSTAR prompt structure is:# Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM]Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.Implementing Our Conversational Flow as a Chain in LangChainTo begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain.@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod.Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework:REPHARSING_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #STANDALONE_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question #ROUTER_DECISION_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass:classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}Final WordsThis blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications. article: Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShareIn the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.Designing the Conversation FlowConversational RAG FlowThe conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.Crafting Prompts with the COSTAR FrameworkTo generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.The COSTAR prompt structure is:# Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM]Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.Implementing Our Conversational Flow as a Chain in LangChainTo begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain.@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod.Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework:REPHARSING_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #STANDALONE_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question #ROUTER_DECISION_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass:classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}Final WordsThis blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications. div: Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShareIn the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.Designing the Conversation FlowConversational RAG FlowThe conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.Crafting Prompts with the COSTAR FrameworkTo generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.The COSTAR prompt structure is:# Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM]Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.Implementing Our Conversational Flow as a Chain in LangChainTo begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain.@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod.Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework:REPHARSING_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #STANDALONE_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question #ROUTER_DECISION_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass:classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}Final WordsThis blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications. div: Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShareIn the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.Designing the Conversation FlowConversational RAG FlowThe conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.Crafting Prompts with the COSTAR FrameworkTo generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.The COSTAR prompt structure is:# Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM]Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.Implementing Our Conversational Flow as a Chain in LangChainTo begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain.@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod.Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework:REPHARSING_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #STANDALONE_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question #ROUTER_DECISION_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass:classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}Final WordsThis blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications. section: Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShareIn the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.Designing the Conversation FlowConversational RAG FlowThe conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.Crafting Prompts with the COSTAR FrameworkTo generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.The COSTAR prompt structure is:# Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM]Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.Implementing Our Conversational Flow as a Chain in LangChainTo begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain.@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod.Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework:REPHARSING_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #STANDALONE_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question #ROUTER_DECISION_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass:classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}Final WordsThis blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications. div: Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShareIn the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.Designing the Conversation FlowConversational RAG FlowThe conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.Crafting Prompts with the COSTAR FrameworkTo generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.The COSTAR prompt structure is:# Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM]Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.Implementing Our Conversational Flow as a Chain in LangChainTo begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain.@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod.Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework:REPHARSING_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #STANDALONE_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question #ROUTER_DECISION_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass:classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}Final WordsThis blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications. div: Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShareIn the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.Designing the Conversation FlowConversational RAG FlowThe conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.Crafting Prompts with the COSTAR FrameworkTo generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.The COSTAR prompt structure is:# Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM]Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.Implementing Our Conversational Flow as a Chain in LangChainTo begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain.@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod.Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework:REPHARSING_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #STANDALONE_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question #ROUTER_DECISION_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass:classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}Final WordsThis blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications. div: Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShareIn the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.Designing the Conversation FlowConversational RAG FlowThe conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.Crafting Prompts with the COSTAR FrameworkTo generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.The COSTAR prompt structure is:# Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM]Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.Implementing Our Conversational Flow as a Chain in LangChainTo begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain.@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod.Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework:REPHARSING_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #STANDALONE_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question #ROUTER_DECISION_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass:classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}Final WordsThis blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications. div: Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShareIn the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.Designing the Conversation FlowConversational RAG FlowThe conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.Crafting Prompts with the COSTAR FrameworkTo generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.The COSTAR prompt structure is:# Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM]Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.Implementing Our Conversational Flow as a Chain in LangChainTo begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain.@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod.Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework:REPHARSING_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #STANDALONE_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question #ROUTER_DECISION_PROMPT# Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format #With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass:classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}Final WordsThis blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications. div: Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChainTal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShare h1: Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChain div: Tal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShare div: Tal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShare div: Tal Waitzenberg·Follow9 min read·Mar 12, 2024185ListenShare div: Tal Waitzenberg·Follow9 min read·Mar 12, 2024 div: Tal Waitzenberg·Follow9 min read·Mar 12, 2024 div: Tal Waitzenberg·Follow div: Tal Waitzenberg·Follow span: Tal Waitzenberg·Follow div: Tal Waitzenberg·Follow div: Tal Waitzenberg·Follow div: Tal Waitzenberg div: Tal Waitzenberg div: Tal Waitzenberg p: Tal Waitzenberg a: Tal Waitzenberg span: · span: · p: Follow span: Follow a: Follow div: 9 min read·Mar 12, 2024 span: 9 min read·Mar 12, 2024 div: 9 min read·Mar 12, 2024 span: 9 min read·Mar 12, 2024 div: 9 min read·Mar 12, 2024 span: 9 min read div: · span: · span: · span: Mar 12, 2024 div: 185ListenShare div: 185 div: 185 div: 185 div: 185 div: 185 div: 185 p: 185 button: 185 div: ListenShare div: Listen div: Listen div: Listen div: Listen div: Listen span: Listen a: Listen div: Listen div: Listen button: Listen div: Listen p: Listen div: Share div: Share div: Share button: Share div: Share p: Share p: In the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries. p: Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction. h2: Designing the Conversation Flow strong: Designing the Conversation Flow figure: Conversational RAG Flow figcaption: Conversational RAG Flow strong: Conversational RAG Flow p: The conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps: p: Question Reshaping Decision —The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context. strong: Question Reshaping Decision — p: Standalone Question Generation —If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors. strong: Standalone Question Generation — p: Inner Router Decision —Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps: strong: Inner Router Decision — ol: Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer. li: Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information. strong: Vectorstore Relevance Check li: LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer. strong: LLM Evaluation ul: RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge. li: RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \"no answer\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information. em: RAG Application Route li: Chat Model Route:If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge. em: Chat Model Route: p: This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query. p: Chat Model —If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly. strong: Chat Model — p: RAG Application —When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query. strong: RAG Application — p: By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses. h2: Crafting Prompts with the COSTAR Framework strong: Crafting Prompts with the COSTAR Framework p: To generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output. p: The COSTAR prompt structure is: pre: # Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM] span: # Context #[Provide relevant background information and context for the prompt]########## Objective #[Clearly state the goal or purpose of the prompt]########## Style #[Specify the desired writing style for the response]########## Tone #[Describe the intended tone or voice for the response]########## Audience #[Identify the target audience for the response]########## Response #[Specify the type of response expected from the LLM] p: Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience. h2: Implementing Our Conversational Flow as a Chain in LangChain p: To begin, let’s implement our customConversationalRagChainusing thefrom_llmmethod of a LangChain Chain. code: ConversationalRagChain code: from_llm pre: @classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,) span: @classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,) span: @classmethod span: def span: from_llm span: cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any, span: None span: Any span: \"\"\"Initialize from LLM.\"\"\" span: # create the rephrasing chain span: # create the standalone question chain span: # router decision chain span: return p: In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models). p: I have created three new chains:rephrasing_chain,standalone_question_chain, androuter_decision_chain, which will be utilized in the_callmethod. code: rephrasing_chain code: standalone_question_chain code: router_decision_chain code: _call p: Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object. p: Now let’s create the prompts templates we used in thefrom_llmmethod using COSTAR framework: code: from_llm ul: REPHARSING_PROMPT li: REPHARSING_PROMPT code: REPHARSING_PROMPT pre: # Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format # span: # Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.########## Style #The response should be clear, concise, and in the form of a straightforward decision - either \"Reshape required\" or \"No reshaping required\".########## Tone #Professional and analytical.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format # ul: STANDALONE_PROMPT li: STANDALONE_PROMPT code: STANDALONE_PROMPT pre: # Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question # span: # Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Take the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.########## Style #The reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.########## Tone #Neutral and focused on accurately capturing the essence of the original question.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.If no reshaping is required, simply output the original question as is.################### Chat History #{chat_history}########## User original question #{ question }########## The new Standalone question # ul: ROUTER_DECISION_PROMPT li: ROUTER_DECISION_PROMPT code: ROUTER_DECISION_PROMPT pre: # Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format # span: # Context #This is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions.########## Objective #Evaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.########## Style #The response should be a clear and direct decision, stated concisely.########## Tone #Analytical and objective.########## Audience #The audience is the internal system components that will act on the decision.########## Response #If the question should be rephrased return response in YAML file format:```result: true```otherwise return in YAML file format:```result: false```################### Chat History #{chat_history}########## User question #{ question }########## Your Decision in YAML format # p: With the prompts defined using the COSTAR framework, we can proceed to implement the_callmethod and complete theConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established. code: _call code: ConversationalRagChain pre: def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer} span: def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer} span: def span: _call span: self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None span: Dict span: str span: Any span: Optional span: None span: Dict span: str span: Any span: \"\"\"Call the chain.\"\"\" span: # chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain. span: if span: not span: \"query\" span: 'result' span: return span: # first check for question rephrasing and then check for standalone question span: \"chat_history\" span: \"question\" span: 'text' span: # parse the llm response using the output parser span: # If the question need rephrasing, return the original question span: if span: # Combine previous chat history and question into a single prompt to generate a standalone question. span: \"chat_history\" span: \"question\" span: 'text' span: # check if there are relevant sources in the vectorstore span: if span: # use the RAG model span: \"query\" span: 'result' span: else span: # send the question to inner router decision, Inner routing --> use chat model or RAG model span: \"chat_history\" span: \"question\" span: \"text\" span: # parse the llm response using the output parser span: if span: # Use chat model span: \"role\" span: \"user\" span: \"content\" span: else span: # get the response from the RAG chain span: \"query\" span: 'result' span: return p: After defining the prompts and implementing the_callmethod, we can construct theConversationalRagChainclass: code: _call code: ConversationalRagChain pre: classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer} span: classConversationalRagChain(Chain):\"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\"rag_chain: Chainrephrasing_chain: LLMChainstandalone_question_chain: LLMChainrouter_decision_chain: LLMChainyaml_output_parser: YamlOutputParser# input\\output parametersinput_key:str=\"query\"chat_history_key:str=\"chat_history\"output_key:str=\"result\"@propertydefinput_keys(self) ->List[str]:\"\"\"Input keys.\"\"\"return[self.input_key, self.chat_history_key]@propertydefoutput_keys(self) ->List[str]:\"\"\"Output keys.\"\"\"return[self.output_key]@propertydef_chain_type(self) ->str:\"\"\"Return the chain type.\"\"\"return\"ConversationalRagChain\"@classmethoddeffrom_llm(cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any,) -> ConversationalRagChain:\"\"\"Initialize from LLM.\"\"\"# create the rephrasing chainrephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)# create the standalone question chainstandalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)# router decision chainrouter_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)returncls(rag_chain=rag_chain,rephrasing_chain=rephrasing_chain,standalone_question_chain=standalone_question_chain,router_decision_chain=router_decision_chain,yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),callbacks=callbacks,**kwargs,)def_call(self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None) ->Dict[str,Any]:\"\"\"Call the chain.\"\"\"chat_history = inputs[self.chat_history_key]question = inputs[self.input_key]# chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.ifnotchat_history:answer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer}# first check for question rephrasing and then check for standalone questionneeds_rephrasing = self.rephrasing_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# parse the llm response using the output parserrephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)# If the question need rephrasing, return the original questionifrephrasing_decision.result:# Combine previous chat history and question into a single prompt to generate a standalone question.standalone_question = self.standalone_question_chain.invoke({\"chat_history\": chat_history,\"question\": question})['text'].strip()# check if there are relevant sources in the vectorstoredocs = self.rag_chain._get_docs(question)ifdocs:# use the RAG modelanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()else:# send the question to inner router decision, Inner routing --> use chat model or RAG modelrouting_decision = self.router_decision_chain.invoke({\"chat_history\": chat_history,\"question\": question},)[\"text\"].strip()# parse the llm response using the output parserrouting_decision = self.yaml_output_parser.parse(routing_decision)ifrouting_decision.result:# Use chat modelanswer = self.llm.invoke(chat_history + [{\"role\":\"user\",\"content\": question}],).contentelse:# get the response from the RAG chainanswer = self.rag_chain.invoke({\"query\": question})['result'].strip()return{self.output_key: answer} span: class span: ConversationalRagChain span: Chain span: \"\"\"Chain that encpsulate RAG application enablingnatural conversations\"\"\" span: # input\\output parameters span: str span: \"query\" span: str span: \"chat_history\" span: str span: \"result\" span: @property span: def span: input_keys span: self span: List span: str span: \"\"\"Input keys.\"\"\" span: return span: @property span: def span: output_keys span: self span: List span: str span: \"\"\"Output keys.\"\"\" span: return span: @property span: def span: _chain_type span: self span: str span: \"\"\"Return the chain type.\"\"\" span: return span: \"ConversationalRagChain\" span: @classmethod span: def span: from_llm span: cls,rag_chain: Chain,llm: BaseLanguageModel,callbacks: Callbacks =None,**kwargs:Any, span: None span: Any span: \"\"\"Initialize from LLM.\"\"\" span: # create the rephrasing chain span: # create the standalone question chain span: # router decision chain span: return span: def span: _call span: self, inputs:Dict[str,Any], run_manager:Optional[CallbackManagerForChainRun] =None span: Dict span: str span: Any span: Optional span: None span: Dict span: str span: Any span: \"\"\"Call the chain.\"\"\" span: # chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain. span: if span: not span: \"query\" span: 'result' span: return span: # first check for question rephrasing and then check for standalone question span: \"chat_history\" span: \"question\" span: 'text' span: # parse the llm response using the output parser span: # If the question need rephrasing, return the original question span: if span: # Combine previous chat history and question into a single prompt to generate a standalone question. span: \"chat_history\" span: \"question\" span: 'text' span: # check if there are relevant sources in the vectorstore span: if span: # use the RAG model span: \"query\" span: 'result' span: else span: # send the question to inner router decision, Inner routing --> use chat model or RAG model span: \"chat_history\" span: \"question\" span: \"text\" span: # parse the llm response using the output parser span: if span: # Use chat model span: \"role\" span: \"user\" span: \"content\" span: else span: # get the response from the RAG chain span: \"query\" span: 'result' span: return h2: Final Words p: This blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts. p: Stay tuned for more insightful posts in my \"Mastering RAG Chatbots\" series, where I am exploring advanced conversational AI techniques and real-world applications. strong: Mastering RAG Chatbots div: Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/month div: Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/month div: Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/month div: Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/month div: Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/month div: Sign up to discover human stories that deepen your understanding of the world. h2: Sign up to discover human stories that deepen your understanding of the world. div: FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/month div: FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for free div: FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience. div: Free div: Free h2: Free div: Distraction-free reading. No ads. p: Distraction-free reading. No ads. div: Organize your knowledge with lists and highlights. p: Organize your knowledge with lists and highlights. div: Tell your story. Find your audience. p: Tell your story. Find your audience. div: Sign up for free span: Sign up for free a: Sign up for free div: MembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for $5/month div: MembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium app div: Membership div: Membership h2: Membership div: Read member-only stories p: Read member-only stories div: Support writers you read most p: Support writers you read most div: Earn money for your writing p: Earn money for your writing div: Listen to audio narrations p: Listen to audio narrations div: Read offline with the Medium app p: Read offline with the Medium app div: Try for $5/month span: Try for $5/month a: Try for $5/month div: GenaiLangchainConversational AIRetrieval AugmentedLlm div: GenaiLangchainConversational AIRetrieval AugmentedLlm div: GenaiLangchainConversational AIRetrieval AugmentedLlm div: Genai a: Genai div: Genai div: Langchain a: Langchain div: Langchain div: Conversational AI a: Conversational AI div: Conversational AI div: Retrieval Augmented a: Retrieval Augmented div: Retrieval Augmented div: Llm a: Llm div: Llm footer: 185185 div: 185185 div: 185185 div: 185185 div: 185185 div: 185185 div: 185185 span: 185 div: 185 div: 185 div: 185 div: 185 p: 185 button: 185 span: 185 div: 185 div: 185 div: 185 div: 185 p: 185 button: 185 div: FollowWritten byTal Waitzenberg69 FollowersI am a pioneer in Generative AI and Data Science, blending AI and ML to craft cross-domain solutions. My expertise - leveraging Classical ML, AI and LLMs..FollowMore from Tal WaitzenbergTal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285See all from Tal WaitzenbergRecommended from MediumMingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285ListsNatural Language Processing1662 stories·1237 savesChatGPT prompts48 stories·1930 savesGenerative AI Recommended Reading52 stories·1302 savesStaff Picks718 stories·1248 savesVijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams div: FollowWritten byTal Waitzenberg69 FollowersI am a pioneer in Generative AI and Data Science, blending AI and ML to craft cross-domain solutions. My expertise - leveraging Classical ML, AI and LLMs..Follow div: FollowWritten byTal Waitzenberg69 FollowersI am a pioneer in Generative AI and Data Science, blending AI and ML to craft cross-domain solutions. My expertise - leveraging Classical ML, AI and LLMs..Follow div: Follow div: Follow div: Follow span: Follow a: Follow div: Written byTal Waitzenberg69 FollowersI am a pioneer in Generative AI and Data Science, blending AI and ML to craft cross-domain solutions. My expertise - leveraging Classical ML, AI and LLMs..Follow div: Written byTal Waitzenberg69 FollowersI am a pioneer in Generative AI and Data Science, blending AI and ML to craft cross-domain solutions. My expertise - leveraging Classical ML, AI and LLMs.. div: Written byTal Waitzenberg a: Written byTal Waitzenberg h2: Written byTal Waitzenberg span: Written byTal Waitzenberg div: 69 Followers div: 69 Followers span: 69 Followers a: 69 Followers div: I am a pioneer in Generative AI and Data Science, blending AI and ML to craft cross-domain solutions. My expertise - leveraging Classical ML, AI and LLMs.. p: I am a pioneer in Generative AI and Data Science, blending AI and ML to craft cross-domain solutions. My expertise - leveraging Classical ML, AI and LLMs.. span: I am a pioneer in Generative AI and Data Science, blending AI and ML to craft cross-domain solutions. My expertise - leveraging Classical ML, AI and LLMs.. div: Follow div: Follow span: Follow a: Follow div: More from Tal WaitzenbergTal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285See all from Tal Waitzenberg div: More from Tal WaitzenbergTal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285See all from Tal Waitzenberg div: More from Tal Waitzenberg h2: More from Tal Waitzenberg div: Tal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912 div: Tal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912 article: Tal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912 div: Tal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912 div: Tal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912 div: Tal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912 div: Tal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912 div: Tal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912 div: Tal WaitzenbergMastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for…Mar 29912 div: Tal Waitzenberg div: Tal Waitzenberg div: Tal Waitzenberg div: Tal Waitzenberg a: Tal Waitzenberg p: Tal Waitzenberg div: Mastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for… div: Mastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for… a: Mastering RAG Chatbots: Semantic Router — RAG gateway— Part 1Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for… div: Mastering RAG Chatbots: Semantic Router — RAG gateway— Part 1 h2: Mastering RAG Chatbots: Semantic Router — RAG gateway— Part 1 div: Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for… h3: Welcome to the second post in the “Mastering RAG Chatbots” series, where we delve into the powerful concept of semantic router for… span: Mar 29912 div: Mar 29912 div: Mar 29912 span: Mar 29 div: 912 div: 912 a: 912 div: 91 div: 91 div: 91 div: 91 span: 91 div: 2 div: 2 div: 2 div: 2 span: 2 div: Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056 div: Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056 article: Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056 div: Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056 div: Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056 div: Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056 div: Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056 div: Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056 div: Tal WaitzenbergMastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in…Apr 3056 div: Tal Waitzenberg div: Tal Waitzenberg div: Tal Waitzenberg div: Tal Waitzenberg a: Tal Waitzenberg p: Tal Waitzenberg div: Mastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in… div: Mastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in… a: Mastering RAG Chabots: Semantic Router — User IntentsIn my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in… div: Mastering RAG Chabots: Semantic Router — User Intents h2: Mastering RAG Chabots: Semantic Router — User Intents div: In my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in… h3: In my third post in the “Mastering RAG Chatbots” series, we will explore the use of semantic routers for intent classification in… span: Apr 3056 div: Apr 3056 div: Apr 3056 span: Apr 30 div: 56 div: 56 a: 56 div: 56 div: 56 div: 56 div: 56 span: 56 div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 article: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal Waitzenberg div: Tal Waitzenberg div: Tal Waitzenberg div: Tal Waitzenberg a: Tal Waitzenberg p: Tal Waitzenberg div: Mastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models… div: Mastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models… a: Mastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models… div: Mastering RAG Chabots: The Road from a POC to Production h2: Mastering RAG Chabots: The Road from a POC to Production div: In my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models… h3: In my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models… span: May 285 div: May 285 div: May 285 span: May 28 div: 5 div: 5 a: 5 div: 5 div: 5 div: 5 div: 5 span: 5 div: See all from Tal Waitzenberg a: See all from Tal Waitzenberg div: See all from Tal Waitzenberg div: Recommended from MediumMingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285ListsNatural Language Processing1662 stories·1237 savesChatGPT prompts48 stories·1930 savesGenerative AI Recommended Reading52 stories·1302 savesStaff Picks718 stories·1248 savesVijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622See more recommendations div: Recommended from MediumMingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285ListsNatural Language Processing1662 stories·1237 savesChatGPT prompts48 stories·1930 savesGenerative AI Recommended Reading52 stories·1302 savesStaff Picks718 stories·1248 savesVijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622See more recommendations div: Recommended from MediumMingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285ListsNatural Language Processing1662 stories·1237 savesChatGPT prompts48 stories·1930 savesGenerative AI Recommended Reading52 stories·1302 savesStaff Picks718 stories·1248 savesVijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622See more recommendations h2: Recommended from Medium div: MingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: MingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: MingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9 div: MingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9 article: MingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9 div: MingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9 div: MingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9 div: MingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9 div: MingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9 div: MingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9 div: MingComparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code!Jan 111.2K9 div: Ming div: Ming div: Ming div: Ming a: Ming p: Ming div: Comparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code! div: Comparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code! a: Comparing LangChain and LlamaIndex with 4 tasksLangChain v.s. LlamaIndex — How do they compare? Show me the code! div: Comparing LangChain and LlamaIndex with 4 tasks h2: Comparing LangChain and LlamaIndex with 4 tasks div: LangChain v.s. LlamaIndex — How do they compare? Show me the code! h3: LangChain v.s. LlamaIndex — How do they compare? Show me the code! span: Jan 111.2K9 div: Jan 111.2K9 div: Jan 111.2K9 span: Jan 11 div: 1.2K9 div: 1.2K9 a: 1.2K9 div: 1.2K div: 1.2K div: 1.2K div: 1.2K span: 1.2K div: 9 div: 9 div: 9 div: 9 span: 9 div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 article: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal WaitzenbergMastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models…May 285 div: Tal Waitzenberg div: Tal Waitzenberg div: Tal Waitzenberg div: Tal Waitzenberg a: Tal Waitzenberg p: Tal Waitzenberg div: Mastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models… div: Mastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models… a: Mastering RAG Chabots: The Road from a POC to ProductionIn my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models… div: Mastering RAG Chabots: The Road from a POC to Production h2: Mastering RAG Chabots: The Road from a POC to Production div: In my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models… h3: In my “Mastering RAG Chatbots” series, we’ve been exploring the details of building advanced Retrieval-Augmented Generation (RAG) models… span: May 285 div: May 285 div: May 285 span: May 28 div: 5 div: 5 a: 5 div: 5 div: 5 div: 5 div: 5 span: 5 h2: Lists div: Natural Language Processing1662 stories·1237 savesChatGPT prompts48 stories·1930 savesGenerative AI Recommended Reading52 stories·1302 savesStaff Picks718 stories·1248 saves div: Natural Language Processing1662 stories·1237 savesChatGPT prompts48 stories·1930 savesGenerative AI Recommended Reading52 stories·1302 savesStaff Picks718 stories·1248 saves div: Natural Language Processing1662 stories·1237 saves a: Natural Language Processing1662 stories·1237 saves div: Natural Language Processing1662 stories·1237 saves h2: Natural Language Processing div: 1662 stories·1237 saves span: · span: · div: ChatGPT prompts48 stories·1930 saves a: ChatGPT prompts48 stories·1930 saves div: ChatGPT prompts48 stories·1930 saves h2: ChatGPT prompts div: 48 stories·1930 saves span: · span: · div: Generative AI Recommended Reading52 stories·1302 saves a: Generative AI Recommended Reading52 stories·1302 saves div: Generative AI Recommended Reading52 stories·1302 saves h2: Generative AI Recommended Reading div: 52 stories·1302 saves span: · span: · div: Staff Picks718 stories·1248 saves a: Staff Picks718 stories·1248 saves div: Staff Picks718 stories·1248 saves h2: Staff Picks div: 718 stories·1248 saves span: · span: · div: Vijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622 div: Vijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928 div: Vijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928 article: Vijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928 div: Vijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928 div: Vijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928 div: Vijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928 div: Vijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928 div: Vijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928 div: Vijaykumar KarthaBeginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval…Apr 2928 div: Vijaykumar Kartha div: Vijaykumar Kartha div: Vijaykumar Kartha div: Vijaykumar Kartha a: Vijaykumar Kartha p: Vijaykumar Kartha div: Beginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval… div: Beginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval… a: Beginner’s Guide To Conversational Retrieval Chain Using LangChainIn the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval… div: Beginner’s Guide To Conversational Retrieval Chain Using LangChain h2: Beginner’s Guide To Conversational Retrieval Chain Using LangChain div: In the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval… h3: In the last article, we created a retrieval chain that can answer only single questions. Let’s now learn about Conversational Retrieval… span: Apr 2928 div: Apr 2928 div: Apr 2928 span: Apr 29 div: 28 div: 28 a: 28 div: 28 div: 28 div: 28 div: 28 span: 28 div: Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131 div: Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131 article: Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131 div: Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131 div: Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131 div: Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131 div: Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131 div: Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131 div: Eric VaillancourtMastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter InteractionsJun 1131 div: Eric Vaillancourt div: Eric Vaillancourt div: Eric Vaillancourt div: Eric Vaillancourt a: Eric Vaillancourt p: Eric Vaillancourt div: Mastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter Interactions div: Mastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter Interactions a: Mastering LangChain RAG: Integrating Chat History (Part 2)Enhancing Q&A Applications: Integrating Historical Context for Smarter Interactions div: Mastering LangChain RAG: Integrating Chat History (Part 2) h2: Mastering LangChain RAG: Integrating Chat History (Part 2) div: Enhancing Q&A Applications: Integrating Historical Context for Smarter Interactions h3: Enhancing Q&A Applications: Integrating Historical Context for Smarter Interactions span: Jun 1131 div: Jun 1131 div: Jun 1131 span: Jun 1 div: 131 div: 131 a: 131 div: 13 div: 13 div: 13 div: 13 span: 13 div: 1 div: 1 div: 1 div: 1 span: 1 div: Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611 div: Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611 article: Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611 div: Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611 div: Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611 div: Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611 div: Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611 div: Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611 div: Suraj BansalBuild Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroductionMay 611 div: Suraj Bansal div: Suraj Bansal div: Suraj Bansal div: Suraj Bansal a: Suraj Bansal p: Suraj Bansal div: Build Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroduction div: Build Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroduction a: Build Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChainIntroduction div: Build Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChain h2: Build Your Own AI Chatbot: A Beginner’s Guide to RAG and LangChain div: Introduction h3: Introduction span: May 611 div: May 611 div: May 611 span: May 6 div: 11 div: 11 a: 11 div: 11 div: 11 div: 11 div: 11 span: 11 div: Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622 div: Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622 article: Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622 div: Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622 div: Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622 div: Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622 div: Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622 div: Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622 div: Atul KumarIncorporating Chat History in Conversational Agents using LangChainIntroductionMay 2622 div: Atul Kumar div: Atul Kumar div: Atul Kumar div: Atul Kumar a: Atul Kumar p: Atul Kumar div: Incorporating Chat History in Conversational Agents using LangChainIntroduction div: Incorporating Chat History in Conversational Agents using LangChainIntroduction a: Incorporating Chat History in Conversational Agents using LangChainIntroduction div: Incorporating Chat History in Conversational Agents using LangChain h2: Incorporating Chat History in Conversational Agents using LangChain div: Introduction h3: Introduction span: May 2622 div: May 2622 div: May 2622 span: May 26 div: 22 div: 22 a: 22 div: 22 div: 22 div: 22 div: 22 span: 22 a: See more recommendations div: See more recommendations div: HelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams div: HelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams div: HelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams div: HelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams div: Help a: Help p: Help div: Status a: Status p: Status div: About a: About p: About div: Careers a: Careers p: Careers div: Press a: Press p: Press div: Blog a: Blog p: Blog div: Privacy a: Privacy p: Privacy div: Terms a: Terms p: Terms div: Text to speech a: Text to speech p: Text to speech div: Teams a: Teams p: Teams script: window.__BUILD_ID__=\"main-20240823-202131-3fcf13c6d9\" script: window.__GRAPHQL_URI__ = \"https://talwaitzenberg.com/_/graphql\" script: window.__PRELOADED_STATE__ = {\"algolia\":{\"queries\":{}},\"cache\":{\"experimentGroupSet\":true,\"reason\":\"This request is not using the cache middleware worker\",\"group\":\"disabled\",\"tags\":[\"group-edgeCachePosts\",\"post-d740493ff328\",\"user-55ec5990cf02\"],\"serverVariantState\":\"\",\"middlewareEnabled\":false,\"cacheStatus\":\"DYNAMIC\",\"shouldUseCache\":false,\"vary\":[],\"lohpSummerUpsellEnabled\":false,\"logoUpdatePhase3Enabled\":false},\"client\":{\"hydrated\":false,\"isUs\":false,\"isNativeMedium\":false,\"isSafariMobile\":false,\"isSafari\":false,\"isFirefox\":false,\"routingEntity\":{\"type\":\"USER\",\"id\":\"55ec5990cf02\",\"explicit\":true},\"viewerIsBot\":false},\"debug\":{\"requestId\":\"e368c4a8-d20a-4bcc-a2a8-e203a9c8ece9\",\"hybridDevServices\":[],\"originalSpanCarrier\":{}},\"multiVote\":{\"clapsPerPost\":{}},\"navigation\":{\"branch\":{\"show\":null,\"hasRendered\":null,\"blockedByCTA\":false},\"hideGoogleOneTap\":false,\"hasRenderedAlternateUserBanner\":null,\"currentLocation\":\"https:\\u002F\\u002Ftalwaitzenberg.com\\u002Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328\",\"host\":\"talwaitzenberg.com\",\"hostname\":\"talwaitzenberg.com\",\"referrer\":\"\",\"hasSetReferrer\":false,\"susiModal\":{\"step\":null,\"operation\":\"register\"},\"postRead\":false,\"partnerProgram\":{\"selectedCountryCode\":null}},\"config\":{\"nodeEnv\":\"production\",\"version\":\"main-20240823-202131-3fcf13c6d9\",\"target\":\"production\",\"productName\":\"Medium\",\"publicUrl\":\"https:\\u002F\\u002Fcdn-client.medium.com\\u002Flite\",\"authDomain\":\"medium.com\",\"authGoogleClientId\":\"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com\",\"favicon\":\"production\",\"glyphUrl\":\"https:\\u002F\\u002Fglyph.medium.com\",\"branchKey\":\"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm\",\"algolia\":{\"appId\":\"MQ57UUUQZ2\",\"apiKeySearch\":\"394474ced050e3911ae2249ecc774921\",\"indexPrefix\":\"medium_\",\"host\":\"-dsn.algolia.net\"},\"recaptchaKey\":\"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk\",\"recaptcha3Key\":\"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5\",\"recaptchaEnterpriseKeyId\":\"6Le-uGgpAAAAAPprRaokM8AKthQ9KNGdoxaGUvVp\",\"datadog\":{\"applicationId\":\"6702d87d-a7e0-42fe-bbcb-95b469547ea0\",\"clientToken\":\"pub853ea8d17ad6821d9f8f11861d23dfed\",\"rumToken\":\"pubf9cc52896502b9413b68ba36fc0c7162\",\"context\":{\"deployment\":{\"target\":\"production\",\"tag\":\"main-20240823-202131-3fcf13c6d9\",\"commit\":\"3fcf13c6d9df70f0ed3900bdc70d81c342e75b4c\"}},\"datacenter\":\"us\"},\"googleAnalyticsCode\":\"G-7JY7T788PK\",\"googlePay\":{\"apiVersion\":\"2\",\"apiVersionMinor\":\"0\",\"merchantId\":\"BCR2DN6TV7EMTGBM\",\"merchantName\":\"Medium\",\"instanceMerchantId\":\"13685562959212738550\"},\"applePay\":{\"version\":3},\"signInWallCustomDomainCollectionIds\":[\"3a8144eabfe3\",\"336d898217ee\",\"61061eb0c96b\",\"138adf9c44c\",\"819cc2aaeee0\"],\"mediumMastodonDomainName\":\"me.dm\",\"mediumOwnedAndOperatedCollectionIds\":[\"8a9336e5bb4\",\"b7e45b22fec3\",\"193b68bd4fba\",\"8d6b8a439e32\",\"54c98c43354d\",\"3f6ecf56618\",\"d944778ce714\",\"92d2092dc598\",\"ae2a65f35510\",\"1285ba81cada\",\"544c7006046e\",\"fc8964313712\",\"40187e704f1c\",\"88d9857e584e\",\"7b6769f2748b\",\"bcc38c8f6edf\",\"cef6983b292\",\"cb8577c9149e\",\"444d13b52878\",\"713d7dbc99b0\",\"ef8e90590e66\",\"191186aaafa0\",\"55760f21cdc5\",\"9dc80918cc93\",\"bdc4052bbdba\",\"8ccfed20cbb2\"],\"tierOneDomains\":[\"medium.com\",\"thebolditalic.com\",\"arcdigital.media\",\"towardsdatascience.com\",\"uxdesign.cc\",\"codeburst.io\",\"psiloveyou.xyz\",\"writingcooperative.com\",\"entrepreneurshandbook.co\",\"prototypr.io\",\"betterhumans.coach.me\",\"theascent.pub\"],\"topicsToFollow\":[\"d61cf867d93f\",\"8a146bc21b28\",\"1eca0103fff3\",\"4d562ee63426\",\"aef1078a3ef5\",\"e15e46793f8d\",\"6158eb913466\",\"55f1c20aba7a\",\"3d18b94f6858\",\"4861fee224fd\",\"63c6f1f93ee\",\"1d98b3a9a871\",\"decb52b64abf\",\"ae5d4995e225\",\"830cded25262\"],\"topicToTagMappings\":{\"accessibility\":\"accessibility\",\"addiction\":\"addiction\",\"android-development\":\"android-development\",\"art\":\"art\",\"artificial-intelligence\":\"artificial-intelligence\",\"astrology\":\"astrology\",\"basic-income\":\"basic-income\",\"beauty\":\"beauty\",\"biotech\":\"biotech\",\"blockchain\":\"blockchain\",\"books\":\"books\",\"business\":\"business\",\"cannabis\":\"cannabis\",\"cities\":\"cities\",\"climate-change\":\"climate-change\",\"comics\":\"comics\",\"coronavirus\":\"coronavirus\",\"creativity\":\"creativity\",\"cryptocurrency\":\"cryptocurrency\",\"culture\":\"culture\",\"cybersecurity\":\"cybersecurity\",\"data-science\":\"data-science\",\"design\":\"design\",\"digital-life\":\"digital-life\",\"disability\":\"disability\",\"economy\":\"economy\",\"education\":\"education\",\"equality\":\"equality\",\"family\":\"family\",\"feminism\":\"feminism\",\"fiction\":\"fiction\",\"film\":\"film\",\"fitness\":\"fitness\",\"food\":\"food\",\"freelancing\":\"freelancing\",\"future\":\"future\",\"gadgets\":\"gadgets\",\"gaming\":\"gaming\",\"gun-control\":\"gun-control\",\"health\":\"health\",\"history\":\"history\",\"humor\":\"humor\",\"immigration\":\"immigration\",\"ios-development\":\"ios-development\",\"javascript\":\"javascript\",\"justice\":\"justice\",\"language\":\"language\",\"leadership\":\"leadership\",\"lgbtqia\":\"lgbtqia\",\"lifestyle\":\"lifestyle\",\"machine-learning\":\"machine-learning\",\"makers\":\"makers\",\"marketing\":\"marketing\",\"math\":\"math\",\"media\":\"media\",\"mental-health\":\"mental-health\",\"mindfulness\":\"mindfulness\",\"money\":\"money\",\"music\":\"music\",\"neuroscience\":\"neuroscience\",\"nonfiction\":\"nonfiction\",\"outdoors\":\"outdoors\",\"parenting\":\"parenting\",\"pets\":\"pets\",\"philosophy\":\"philosophy\",\"photography\":\"photography\",\"podcasts\":\"podcast\",\"poetry\":\"poetry\",\"politics\":\"politics\",\"privacy\":\"privacy\",\"product-management\":\"product-management\",\"productivity\":\"productivity\",\"programming\":\"programming\",\"psychedelics\":\"psychedelics\",\"psychology\":\"psychology\",\"race\":\"race\",\"relationships\":\"relationships\",\"religion\":\"religion\",\"remote-work\":\"remote-work\",\"san-francisco\":\"san-francisco\",\"science\":\"science\",\"self\":\"self\",\"self-driving-cars\":\"self-driving-cars\",\"sexuality\":\"sexuality\",\"social-media\":\"social-media\",\"society\":\"society\",\"software-engineering\":\"software-engineering\",\"space\":\"space\",\"spirituality\":\"spirituality\",\"sports\":\"sports\",\"startups\":\"startup\",\"style\":\"style\",\"technology\":\"technology\",\"transportation\":\"transportation\",\"travel\":\"travel\",\"true-crime\":\"true-crime\",\"tv\":\"tv\",\"ux\":\"ux\",\"venture-capital\":\"venture-capital\",\"visual-design\":\"visual-design\",\"work\":\"work\",\"world\":\"world\",\"writing\":\"writing\"},\"defaultImages\":{\"avatar\":{\"imageId\":\"1*dmbNkD5D-u45r44go_cf0g.png\",\"height\":150,\"width\":150},\"orgLogo\":{\"imageId\":\"7*V1_7XP4snlmqrc_0Njontw.png\",\"height\":110,\"width\":500},\"postLogo\":{\"imageId\":\"bd978bb536350a710e8efb012513429cabdc4c28700604261aeda246d0f980b7\",\"height\":810,\"width\":1440},\"postPreviewImage\":{\"imageId\":\"1*hn4v1tCaJy7cWMyb0bpNpQ.png\",\"height\":386,\"width\":579}},\"collectionStructuredData\":{\"8d6b8a439e32\":{\"name\":\"Elemental\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Farticles\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\u002F\\u002Fcdn-images-1.medium.com\\u002Fmax\\u002F980\\u002F1*9ygdqoKprhwuTVKUM0DLPA@2x.png\",\"width\":980,\"height\":159}}},\"3f6ecf56618\":{\"name\":\"Forge\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Farticles\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\u002F\\u002Fcdn-images-1.medium.com\\u002Fmax\\u002F596\\u002F1*uULpIlImcO5TDuBZ6lm7Lg@2x.png\",\"width\":596,\"height\":183}}},\"ae2a65f35510\":{\"name\":\"GEN\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Farticles\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\u002F\\u002Fmiro.medium.com\\u002Fmax\\u002F264\\u002F1*RdVZMdvfV3YiZTw6mX7yWA.png\",\"width\":264,\"height\":140}}},\"88d9857e584e\":{\"name\":\"LEVEL\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Farticles\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\u002F\\u002Fmiro.medium.com\\u002Fmax\\u002F540\\u002F1*JqYMhNX6KNNb2UlqGqO2WQ.png\",\"width\":540,\"height\":108}}},\"7b6769f2748b\":{\"name\":\"Marker\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Farticles\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\u002F\\u002Fcdn-images-1.medium.com\\u002Fmax\\u002F383\\u002F1*haCUs0wF6TgOOvfoY-jEoQ@2x.png\",\"width\":383,\"height\":92}}},\"444d13b52878\":{\"name\":\"OneZero\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Farticles\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\u002F\\u002Fmiro.medium.com\\u002Fmax\\u002F540\\u002F1*cw32fIqCbRWzwJaoQw6BUg.png\",\"width\":540,\"height\":123}}},\"8ccfed20cbb2\":{\"name\":\"Zora\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Farticles\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\u002F\\u002Fmiro.medium.com\\u002Fmax\\u002F540\\u002F1*tZUQqRcCCZDXjjiZ4bDvgQ.png\",\"width\":540,\"height\":106}}}},\"embeddedPostIds\":{\"coronavirus\":\"cd3010f9d81f\"},\"sharedCdcMessaging\":{\"COVID_APPLICABLE_TAG_SLUGS\":[],\"COVID_APPLICABLE_TOPIC_NAMES\":[],\"COVID_APPLICABLE_TOPIC_NAMES_FOR_TOPIC_PAGE\":[],\"COVID_MESSAGES\":{\"tierA\":{\"text\":\"For more information on the novel coronavirus and Covid-19, visit cdc.gov.\",\"markups\":[{\"start\":66,\"end\":73,\"href\":\"https:\\u002F\\u002Fwww.cdc.gov\\u002Fcoronavirus\\u002F2019-nCoV\"}]},\"tierB\":{\"text\":\"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.\",\"markups\":[{\"start\":37,\"end\":45,\"href\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Fcategories\\u002F201931128-Policies-Safety\"},{\"start\":125,\"end\":132,\"href\":\"https:\\u002F\\u002Fwww.cdc.gov\\u002Fcoronavirus\\u002F2019-nCoV\"}]},\"paywall\":{\"text\":\"This article has been made free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.\",\"markups\":[{\"start\":56,\"end\":70,\"href\":\"https:\\u002F\\u002Fmedium.com\\u002Fmembership\"},{\"start\":138,\"end\":145,\"href\":\"https:\\u002F\\u002Fwww.cdc.gov\\u002Fcoronavirus\\u002F2019-nCoV\"}]},\"unbound\":{\"text\":\"This article is free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.\",\"markups\":[{\"start\":45,\"end\":59,\"href\":\"https:\\u002F\\u002Fmedium.com\\u002Fmembership\"},{\"start\":127,\"end\":134,\"href\":\"https:\\u002F\\u002Fwww.cdc.gov\\u002Fcoronavirus\\u002F2019-nCoV\"}]}},\"COVID_BANNER_POST_ID_OVERRIDE_WHITELIST\":[\"3b31a67bff4a\"]},\"sharedVoteMessaging\":{\"TAGS\":[\"politics\",\"election-2020\",\"government\",\"us-politics\",\"election\",\"2020-presidential-race\",\"trump\",\"donald-trump\",\"democrats\",\"republicans\",\"congress\",\"republican-party\",\"democratic-party\",\"biden\",\"joe-biden\",\"maga\"],\"TOPICS\":[\"politics\",\"election\"],\"MESSAGE\":{\"text\":\"Find out more about the U.S. election results here.\",\"markups\":[{\"start\":46,\"end\":50,\"href\":\"https:\\u002F\\u002Fcookpolitical.com\\u002F2020-national-popular-vote-tracker\"}]},\"EXCLUDE_POSTS\":[\"397ef29e3ca5\"]},\"embedPostRules\":[],\"recircOptions\":{\"v1\":{\"limit\":3},\"v2\":{\"limit\":8}},\"braintreeClientKey\":\"production_zjkj96jm_m56f8fqpf7ngnrd4\",\"braintree\":{\"enabled\":true,\"merchantId\":\"m56f8fqpf7ngnrd4\",\"merchantAccountId\":{\"usd\":\"AMediumCorporation_instant\",\"eur\":\"amediumcorporation_EUR\",\"cad\":\"amediumcorporation_CAD\"},\"publicKey\":\"ds2nn34bg2z7j5gd\",\"braintreeEnvironment\":\"production\",\"dashboardUrl\":\"https:\\u002F\\u002Fwww.braintreegateway.com\\u002Fmerchants\",\"gracePeriodDurationInDays\":14,\"mediumMembershipPlanId\":{\"monthly\":\"ce105f8c57a3\",\"monthlyV2\":\"e8a5e126-792b-4ee6-8fba-d574c1b02fc5\",\"monthlyWithTrial\":\"d5ee3dbe3db8\",\"monthlyPremium\":\"fa741a9b47a2\",\"yearly\":\"a40ad4a43185\",\"yearlyV2\":\"3815d7d6-b8ca-4224-9b8c-182f9047866e\",\"yearlyStaff\":\"d74fb811198a\",\"yearlyWithTrial\":\"b3bc7350e5c7\",\"yearlyPremium\":\"e21bd2c12166\",\"monthlyOneYearFree\":\"e6c0637a-2bad-4171-ab4f-3c268633d83c\",\"monthly25PercentOffFirstYear\":\"235ecc62-0cdb-49ae-9378-726cd21c504b\",\"monthly20PercentOffFirstYear\":\"ba518864-9c13-4a99-91ca-411bf0cac756\",\"monthly15PercentOffFirstYear\":\"594c029b-9f89-43d5-88f8-8173af4e070e\",\"monthly10PercentOffFirstYear\":\"c6c7bc9a-40f2-4b51-8126-e28511d5bdb0\",\"monthlyForStudents\":\"629ebe51-da7d-41fd-8293-34cd2f2030a8\",\"yearlyOneYearFree\":\"78ba7be9-0d9f-4ece-aa3e-b54b826f2bf1\",\"yearly25PercentOffFirstYear\":\"2dbb010d-bb8f-4eeb-ad5c-a08509f42d34\",\"yearly20PercentOffFirstYear\":\"47565488-435b-47f8-bf93-40d5fbe0ebc8\",\"yearly15PercentOffFirstYear\":\"8259809b-0881-47d9-acf7-6c001c7f720f\",\"yearly10PercentOffFirstYear\":\"9dd694fb-96e1-472c-8d9e-3c868d5c1506\",\"yearlyForStudents\":\"e29345ef-ab1c-4234-95c5-70e50fe6bc23\",\"monthlyCad\":\"p52orjkaceei\",\"yearlyCad\":\"h4q9g2up9ktt\"},\"braintreeDiscountId\":{\"oneMonthFree\":\"MONTHS_FREE_01\",\"threeMonthsFree\":\"MONTHS_FREE_03\",\"sixMonthsFree\":\"MONTHS_FREE_06\",\"fiftyPercentOffOneYear\":\"FIFTY_PERCENT_OFF_ONE_YEAR\"},\"3DSecureVersion\":\"2\",\"defaultCurrency\":\"usd\",\"providerPlanIdCurrency\":{\"4ycw\":\"usd\",\"rz3b\":\"usd\",\"3kqm\":\"usd\",\"jzw6\":\"usd\",\"c2q2\":\"usd\",\"nnsw\":\"usd\",\"q8qw\":\"usd\",\"d9y6\":\"usd\",\"fx7w\":\"cad\",\"nwf2\":\"cad\"}},\"paypalClientId\":\"AXj1G4fotC2GE8KzWX9mSxCH1wmPE3nJglf4Z2ig_amnhvlMVX87otaq58niAg9iuLktVNF_1WCMnN7v\",\"paypal\":{\"host\":\"https:\\u002F\\u002Fapi.paypal.com:443\",\"clientMode\":\"production\",\"serverMode\":\"live\",\"webhookId\":\"4G466076A0294510S\",\"monthlyPlan\":{\"planId\":\"P-9WR0658853113943TMU5FDQA\",\"name\":\"Medium Membership (Monthly) with setup fee\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed monthly.\"},\"yearlyPlan\":{\"planId\":\"P-7N8963881P8875835MU5JOPQ\",\"name\":\"Medium Membership (Annual) with setup fee\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed annually.\"},\"oneYearGift\":{\"name\":\"Medium Membership (1 Year, Digital Gift Code)\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Gift codes can be redeemed at medium.com\\u002Fredeem.\",\"price\":\"50.00\",\"currency\":\"USD\",\"sku\":\"membership-gift-1-yr\"},\"oldMonthlyPlan\":{\"planId\":\"P-96U02458LM656772MJZUVH2Y\",\"name\":\"Medium Membership (Monthly)\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed monthly.\"},\"oldYearlyPlan\":{\"planId\":\"P-59P80963JF186412JJZU3SMI\",\"name\":\"Medium Membership (Annual)\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed annually.\"},\"monthlyPlanWithTrial\":{\"planId\":\"P-66C21969LR178604GJPVKUKY\",\"name\":\"Medium Membership (Monthly) with setup fee\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed monthly.\"},\"yearlyPlanWithTrial\":{\"planId\":\"P-6XW32684EX226940VKCT2MFA\",\"name\":\"Medium Membership (Annual) with setup fee\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed annually.\"},\"oldMonthlyPlanNoSetupFee\":{\"planId\":\"P-4N046520HR188054PCJC7LJI\",\"name\":\"Medium Membership (Monthly)\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed monthly.\"},\"oldYearlyPlanNoSetupFee\":{\"planId\":\"P-7A4913502Y5181304CJEJMXQ\",\"name\":\"Medium Membership (Annual)\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed annually.\"},\"sdkUrl\":\"https:\\u002F\\u002Fwww.paypal.com\\u002Fsdk\\u002Fjs\"},\"stripePublishableKey\":\"pk_live_7FReX44VnNIInZwrIIx6ghjl\",\"log\":{\"json\":true,\"level\":\"info\"},\"imageUploadMaxSizeMb\":25,\"staffPicks\":{\"title\":\"Staff Picks\",\"catalogId\":\"c7bc6e1ee00f\"}},\"session\":{\"xsrf\":\"\"}} script: window.__APOLLO_STATE__ = {\"ROOT_QUERY\":{\"__typename\":\"Query\",\"viewer\":null,\"variantFlags\":[{\"__typename\":\"VariantFlag\",\"name\":\"enable_marketing_emails\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"android_two_hour_refresh\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_deprecate_legacy_providers_v3\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_braintree_client\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_pre_pp_v4\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_android_dynamic_programming_paywall\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_apple_webhook\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_google_one_tap\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_new_partner_program_enrollment_flow\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_premium_tier_badge\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"available_annual_premium_plan\",\"valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"4a442ace1476\"}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_apple_sign_in\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_rex_reading_history\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"ios_enable_lock_responses\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"ios_iceland_nux\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_lo_homepage\",\"valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"control\"}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_mastodon_avatar_upload\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_premium_tier\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"android_enable_friend_links_creation\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"crm_send_contact_to_sendgrid\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_sharer_create_post_share_key\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_susi_redesign_android\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"android_enable_topic_portals\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_simplified_digest_v2_b\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_tick_landing_page\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"signin_services\",\"valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"twitter,facebook,google,email,google-fastidv,google-one-tap,apple\"}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_eventstats_event_processing\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_new_stripe_customers\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_starspace\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"price_smoke_test_yearly\",\"valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"\"}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_rex_new_push_notification_endpoint\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_see_pronouns\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_lite_server_upstream_deadlines\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_moc_load_processor_first_story\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"ios_social_share_sheet\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_moc_load_processor_all_recs_surfaces\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_explicit_signals\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"signup_services\",\"valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"twitter,facebook,google,email,google-fastidv,google-one-tap,apple\"}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_configure_pronouns\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_should_index_post_query\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_speechify_ios\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"onboarding_tags_from_top_views\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"android_rating_prompt_stories_read_threshold\",\"valueType\":{\"__typename\":\"VariantFlagNumber\"}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_entities_to_follow_v2\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"ios_in_app_free_trial\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_verifications_service\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"limit_post_referrers\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_android_miro_v2\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_creator_welcome_email\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"ios_enable_verified_book_author\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_braintree_apple_pay\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_recommended_publishers_query\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_tipping_v0_ios\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_auto_follow_on_subscribe\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_new_manage_membership_flow\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_sprig\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_medium2_kbfd\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"ios_enable_friend_links_postpage_banners\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_lite_homepage\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_author_cards\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_braintree_google_pay\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_recaptcha_enterprise\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_mastodon_for_members_username_selection\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_pp_country_expansion\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_tipping_v0_android\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"price_smoke_test_monthly\",\"valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"\"}},{\"__typename\":\"VariantFlag\",\"name\":\"android_enable_lists_v2\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_lite_response_markup\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"ios_remove_twitter_onboarding_step\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"skip_fs_cache_user_vals\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_group_gifting\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_google_webhook\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"reengagement_notification_duration\",\"valueType\":{\"__typename\":\"VariantFlagNumber\"}},{\"__typename\":\"VariantFlag\",\"name\":\"android_enable_editor_new_publishing_flow\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_conversion_model_v2\",\"valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"group_2\"}},{\"__typename\":\"VariantFlag\",\"name\":\"disable_partner_program_enrollment\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_branch_io\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_sharer_validate_post_share_key\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_updated_digest_previews\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_susi_redesign_ios\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_lite_archive_page\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"allow_signup\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_rex_aggregator_v2\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"redefined_top_posts\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_maim_the_meter\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_ml_rank_rex_anno\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_moc_load_processor_c\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"mobile_custom_app_icon\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"textshots_userid\",\"valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"\"}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_explicit_signals_updated_post_previews\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_ios_autorefresh\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_import\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_aurora_pub_follower_page\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"can_send_tips_v0\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_braintree_integration\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_rito_upstream_deadlines\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_pp_v4\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_ios_easy_resubscribe\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_lite_pub_stats\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_update_topic_portals_wtf\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"available_monthly_plan\",\"valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"60e220181034\"}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_app_flirty_thirty\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_legacy_feed_in_iceland\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_seamless_social_sharing\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"reader_fair_distribution_non_qp\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_tribute_landing_page\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"browsable_stream_config_bucket\",\"valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"curated-topics\"}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_lite_continue_this_thread\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"limit_user_follows\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_automod\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_recirc_model\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_author_cards_byline\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"coronavirus_topic_recirc\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"glyph_font_set\",\"valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"m2-unbound-source-serif-pro\"}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_ios_offline_reading\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_members_only_audio\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"ios_enable_home_post_menu\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_android_dynamic_aspirational_paywall\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_diversification_rex\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"android_enable_image_sharer\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_android_offline_reading\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_speechify_widget\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"ios_enable_friend_links_creation\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"can_receive_tips_v0\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_iceland_forced_android\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"available_annual_plan\",\"valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"2c754bcc2995\"}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_updated_pub_recs_ui\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"android_enable_syntax_highlight\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"android_enable_friend_links_postpage_banners\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_footer_app_buttons\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_ios_dynamic_paywall_aspiriational\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_lite_partner_program_dashboard\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"rex_generator_max_candidates\",\"valueType\":{\"__typename\":\"VariantFlagNumber\"}},{\"__typename\":\"VariantFlag\",\"name\":\"allow_test_auth\",\"valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"disallow\"}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_braintree_paypal\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_ios_dynamic_paywall_programming\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_switch_plan_premium_tier\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_android_verified_author\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_bg_post_post\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_braintree_webhook\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_mastodon_for_members\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_newsletter_lo_flow_custom_domains\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_pill_based_home_feed\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"ios_display_paywall_after_onboarding\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"available_monthly_premium_plan\",\"valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"12a660186432\"}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_braintree_trial_membership\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_cache_less_following_feed\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"enable_tag_recs\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}},{\"__typename\":\"VariantFlag\",\"name\":\"allow_access\",\"valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true}}],\"collectionByDomainOrSlug({\\\"domainOrSlug\\\":\\\"talwaitzenberg.com\\\"})\":null,\"postResult({\\\"id\\\":\\\"d740493ff328\\\"})\":{\"__ref\":\"Post:d740493ff328\"}},\"LinkedAccounts:55ec5990cf02\":{\"__typename\":\"LinkedAccounts\",\"mastodon\":null,\"id\":\"55ec5990cf02\"},\"UserViewerEdge:userId:55ec5990cf02-viewerId:lo_6efb08673bd8\":{\"__typename\":\"UserViewerEdge\",\"id\":\"userId:55ec5990cf02-viewerId:lo_6efb08673bd8\",\"isFollowing\":false,\"isUser\":false,\"isMuting\":false},\"NewsletterV3:c3c8261f66b3\":{\"__typename\":\"NewsletterV3\",\"id\":\"c3c8261f66b3\",\"type\":\"NEWSLETTER_TYPE_AUTHOR\",\"slug\":\"55ec5990cf02\",\"name\":\"55ec5990cf02\",\"collection\":null,\"user\":{\"__ref\":\"User:55ec5990cf02\"}},\"User:55ec5990cf02\":{\"__typename\":\"User\",\"id\":\"55ec5990cf02\",\"name\":\"Tal Waitzenberg\",\"username\":\"talon8080\",\"newsletterV3\":{\"__ref\":\"NewsletterV3:c3c8261f66b3\"},\"linkedAccounts\":{\"__ref\":\"LinkedAccounts:55ec5990cf02\"},\"isSuspended\":false,\"imageId\":\"1*BOmFKnB8tSWKQMRJvzWTKQ.png\",\"mediumMemberAt\":1694691755000,\"verifications\":{\"__typename\":\"VerifiedInfo\",\"isBookAuthor\":false},\"socialStats\":{\"__typename\":\"SocialStats\",\"followerCount\":69},\"customDomainState\":{\"__typename\":\"CustomDomainState\",\"live\":{\"__typename\":\"CustomDomain\",\"domain\":\"talwaitzenberg.com\"}},\"hasSubdomain\":false,\"bio\":\"I am a pioneer in Generative AI and Data Science, blending AI and ML to craft cross-domain solutions. My expertise - leveraging Classical ML, AI and LLMs..\",\"isPartnerProgramEnrolled\":false,\"viewerEdge\":{\"__ref\":\"UserViewerEdge:userId:55ec5990cf02-viewerId:lo_6efb08673bd8\"},\"viewerIsUser\":false,\"postSubscribeMembershipUpsellShownAt\":0,\"allowNotes\":true,\"twitterScreenName\":\"\",\"membership\":{\"__ref\":\"Membership:6d695e0ea6f8\"}},\"Paragraph:415c67eeaf84_0\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_0\",\"name\":\"08d3\",\"type\":\"H3\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChain\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"ImageMetadata:1*Ok5uPTnHHr8WB09oNHl1tQ.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*Ok5uPTnHHr8WB09oNHl1tQ.png\",\"originalHeight\":439,\"originalWidth\":858,\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null},\"Paragraph:415c67eeaf84_1\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_1\",\"name\":\"48ef\",\"type\":\"IMG\",\"href\":null,\"layout\":\"INSET_CENTER\",\"metadata\":{\"__ref\":\"ImageMetadata:1*Ok5uPTnHHr8WB09oNHl1tQ.png\"},\"text\":\"\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_2\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_2\",\"name\":\"8e7e\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"In the rapidly evolving landscape of generative AI, Retrieval Augmented Generation (RAG) models have emerged as powerful tools for leveraging the vast knowledge repositories available to us. However, simply building a RAG model is not enough; the true challenge lies in harnessing its full potential and integrating it seamlessly into real-world applications. This blog post, part of my “Mastering RAG Chatbots” series, delves into the fascinating realm of transforming your RAG model into a conversational AI assistant, acting as an invaluable tool to answer user queries.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_3\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_3\",\"name\":\"ddad\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Through this post, we will explore a simple yet valuable approach to endowing your RAG application with the ability to engage in natural conversations. By defining and implementing a decision mechanism, we will determine when to rely on the RAG’s knowledge retrieval capabilities and when to respond with more casual, conversational responses. Leveraging the power of LangChain, a robust framework for building applications with large language models, we will bring this vision to life, empowering you to create truly advanced conversational AI tools that seamlessly blend knowledge retrieval and natural language interaction.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_4\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_4\",\"name\":\"06ef\",\"type\":\"H4\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Designing the Conversation Flow\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"STRONG\",\"start\":0,\"end\":31,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"ImageMetadata:1*EP0UpwM8cTe6569NPxZcFg.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*EP0UpwM8cTe6569NPxZcFg.png\",\"originalHeight\":439,\"originalWidth\":1078,\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null},\"Paragraph:415c67eeaf84_5\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_5\",\"name\":\"0d43\",\"type\":\"IMG\",\"href\":null,\"layout\":\"INSET_CENTER\",\"metadata\":{\"__ref\":\"ImageMetadata:1*EP0UpwM8cTe6569NPxZcFg.png\"},\"text\":\"Conversational RAG Flow\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"STRONG\",\"start\":0,\"end\":23,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_6\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_6\",\"name\":\"ab6f\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"The conversation flow is a crucial component that governs when to leverage the RAG application and when to rely on the chat model. This decision-making process comprises five distinct steps:\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_7\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_7\",\"name\":\"2fca\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Question Reshaping Decision — The first step evaluates whether the user’s question requires reshaping or can be processed as is. This decision is made by prompting the LLM with the user’s question and relevant context.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"STRONG\",\"start\":0,\"end\":29,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_8\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_8\",\"name\":\"0053\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Standalone Question Generation — If reshaping is necessary, this step transforms the original question into a standalone query that encapsulates all the required context and information. The LLM is prompted to generate a self-contained question that can be understood and answered without relying on external factors.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"STRONG\",\"start\":0,\"end\":32,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_9\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_9\",\"name\":\"5aa5\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Inner Router Decision — Once the question is reshaped into a suitable format, the inner router determines the appropriate path for obtaining a comprehensive answer. This decision-making process involves the following steps:\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"STRONG\",\"start\":0,\"end\":23,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_10\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_10\",\"name\":\"fd39\",\"type\":\"OLI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Vectorstore Relevance Check: The inner router first checks the vectorstore for relevant sources that could potentially answer the reshaped question. If relevant sources are found, the query is forwarded to the RAG application for generating a response based on the retrieved information.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"STRONG\",\"start\":0,\"end\":27,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_11\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_11\",\"name\":\"242f\",\"type\":\"OLI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"LLM Evaluation: If no relevant sources are found in the vectorstore, the reshaped question is prompted to the LLM. The LLM evaluates the query and determines the best path based on the information needed to provide a suitable answer.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"STRONG\",\"start\":0,\"end\":14,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_12\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_12\",\"name\":\"4c75\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"RAG Application Route: Despite the absence of relevant sources in the vectorstore, the LLM may still recommend utilizing the RAG application. In such cases, the RAG application is invoked, and a \\\"no answer\\\" response is returned, indicating that the query cannot be satisfactorily addressed with the available information.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"EM\",\"start\":0,\"end\":21,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_13\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_13\",\"name\":\"a697\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Chat Model Route: If the LLM deems the chat model's capabilities sufficient to address the reshaped question, the query is processed by the chat model, which generates a response based on the conversation history and its inherent knowledge.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"EM\",\"start\":0,\"end\":17,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_14\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_14\",\"name\":\"6838\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"This approach ensures that the inner router leverages the strengths of both the vectorstore, the RAG application, and the chat model. By first checking for relevant sources and then involving the LLM’s decision-making capabilities, the system can provide comprehensive answers when possible or gracefully indicate the lack of sufficient information to address the query.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_15\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_15\",\"name\":\"2933\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Chat Model — If the inner router decides that the chat model can handle the question effectively, it processes the query based on the conversation history and generates a response accordingly.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"STRONG\",\"start\":0,\"end\":12,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_16\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_16\",\"name\":\"7d43\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"RAG Application — When the question demands the retrieval and synthesis of information from a broad knowledge base, the RAG application is invoked. This application utilizes a vector store to search for relevant information and generate an answer tailored to the user’s query.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"STRONG\",\"start\":0,\"end\":17,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_17\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_17\",\"name\":\"1ac7\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"By leveraging the capabilities of an LLM to make decisions in steps 1–3, the conversation flow can dynamically adapt to the user’s input, ensuring that questions are processed efficiently and leveraging the strengths of both the chat model and the RAG application to provide accurate and contextually relevant responses.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_18\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_18\",\"name\":\"19bf\",\"type\":\"H4\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Crafting Prompts with the COSTAR Framework\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"STRONG\",\"start\":0,\"end\":42,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_19\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_19\",\"name\":\"eefc\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"To generate effective prompts for each step in the conversation flow, we’ll utilize the COSTAR framework. COSTAR (Context, Objective, Style, Tone, Audience, Response) offers a structured approach to prompt creation, ensuring all key aspects influencing an LLM’s response are considered for tailored and impactful output.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_20\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_20\",\"name\":\"3252\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"The COSTAR prompt structure is:\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_21\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_21\",\"name\":\"759f\",\"type\":\"PRE\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"# Context #\\n[Provide relevant background information and context for the prompt]\\n\\n#########\\n\\n# Objective #\\n[Clearly state the goal or purpose of the prompt]\\n\\n#########\\n\\n# Style #\\n[Specify the desired writing style for the response]\\n\\n#########\\n\\n# Tone #\\n[Describe the intended tone or voice for the response]\\n\\n#########\\n\\n# Audience #\\n[Identify the target audience for the response]\\n\\n#########\\n\\n# Response #\\n[Specify the type of response expected from the LLM]\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":{\"__typename\":\"CodeBlockMetadata\",\"mode\":\"EXPLICIT\",\"lang\":\"plaintext\"},\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_22\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_22\",\"name\":\"394a\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Utilizing the COSTAR framework ensures that our prompts are comprehensive, clear, and aligned with the intended purpose, enabling the LLM to generate high-quality responses that enhance the overall conversation experience.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_23\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_23\",\"name\":\"59be\",\"type\":\"H4\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Implementing Our Conversational Flow as a Chain in LangChain\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_24\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_24\",\"name\":\"eaef\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"To begin, let’s implement our custom ConversationalRagChainusing the from_llm method of a LangChain Chain.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"CODE\",\"start\":37,\"end\":59,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null},{\"__typename\":\"Markup\",\"type\":\"CODE\",\"start\":69,\"end\":77,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_25\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_25\",\"name\":\"753f\",\"type\":\"PRE\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"@classmethod\\ndef from_llm(\\n        cls,\\n        rag_chain: Chain,\\n        llm: BaseLanguageModel,\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n) -\\u003E ConversationalRagChain:\\n    \\\"\\\"\\\"Initialize from LLM.\\\"\\\"\\\"\\n\\n    # create the rephrasing chain\\n    rephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)\\n\\n    # create the standalone question chain\\n    standalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)\\n\\n    # router decision chain\\n    router_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)\\n\\n    return cls(\\n        rag_chain=rag_chain,\\n        rephrasing_chain=rephrasing_chain,\\n        standalone_question_chain=standalone_question_chain,\\n        router_decision_chain=router_decision_chain,\\n        yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),\\n        callbacks=callbacks,\\n        **kwargs,\\n    )\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":{\"__typename\":\"CodeBlockMetadata\",\"mode\":\"EXPLICIT\",\"lang\":\"python\"},\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_26\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_26\",\"name\":\"3390\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"In this method, we receive two inputs: the RAG chain (which you wish to transform into a conversational RAG, I will demonstrate how to implement an advanced RAG in my next posts), and the LLM (which can be any valid LLM from OpenAI, Google, Anthropic, or any other provider, including open-source models).\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_27\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_27\",\"name\":\"41a6\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"I have created three new chains: rephrasing_chain, standalone_question_chain, and router_decision_chain, which will be utilized in the _call method.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"CODE\",\"start\":33,\"end\":49,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null},{\"__typename\":\"Markup\",\"type\":\"CODE\",\"start\":51,\"end\":76,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null},{\"__typename\":\"Markup\",\"type\":\"CODE\",\"start\":82,\"end\":103,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null},{\"__typename\":\"Markup\",\"type\":\"CODE\",\"start\":135,\"end\":140,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_28\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_28\",\"name\":\"366d\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Additionally, I will be employing LangChain’s output parser to obtain the decisions of the chains in a YAML format, using a pydantic object.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_29\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_29\",\"name\":\"7a2b\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Now let’s create the prompts templates we used in the from_llmmethod using COSTAR framework:\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"CODE\",\"start\":54,\"end\":62,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_30\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_30\",\"name\":\"4200\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"REPHARSING_PROMPT\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"CODE\",\"start\":0,\"end\":17,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_31\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_31\",\"name\":\"f393\",\"type\":\"PRE\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"# Context #\\nThis is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions. \\n\\n#########\\n\\n# Objective #\\nEvaluate the given user question and determine if it requires reshaping according to chat history to provide necessary context and information for answering, or if it can be processed as is.\\n\\n#########\\n\\n# Style #\\nThe response should be clear, concise, and in the form of a straightforward decision - either \\\"Reshape required\\\" or \\\"No reshaping required\\\".\\n\\n#########\\n\\n# Tone #\\nProfessional and analytical.\\n\\n#########\\n\\n# Audience #\\nThe audience is the internal system components that will act on the decision.\\n\\n#########\\n\\n# Response #\\nIf the question should be rephrased return response in YAML file format:\\n```\\n    result: true\\n```\\notherwise return in YAML file format:\\n```\\n    result: false\\n```\\n\\n##################\\n\\n# Chat History #\\n{chat_history}\\n\\n#########\\n\\n# User question #\\n{ question }\\n\\n#########\\n\\n# Your Decision in YAML format # \",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":{\"__typename\":\"CodeBlockMetadata\",\"mode\":\"EXPLICIT\",\"lang\":\"plaintext\"},\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_32\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_32\",\"name\":\"7b73\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"STANDALONE_PROMPT\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"CODE\",\"start\":0,\"end\":17,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_33\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_33\",\"name\":\"21aa\",\"type\":\"PRE\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"# Context #\\nThis is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions. \\n\\n#########\\n\\n# Objective #\\nTake the original user question and chat history, and generate a new standalone question that can be understood and answered without relying on additional external information.\\n\\n#########\\n\\n# Style #\\nThe reshaped standalone question should be clear, concise, and self-contained, while maintaining the intent and meaning of the original query.\\n\\n#########\\n\\n# Tone #\\nNeutral and focused on accurately capturing the essence of the original question.\\n\\n#########\\n\\n# Audience #\\nThe audience is the internal system components that will act on the decision.\\n\\n#########\\n\\n# Response #\\nIf the original question requires reshaping, provide a new reshaped standalone question that includes all necessary context and information to be self-contained.\\nIf no reshaping is required, simply output the original question as is.\\n\\n##################\\n\\n# Chat History #\\n{chat_history}\\n\\n#########\\n\\n# User original question #\\n{ question }\\n\\n#########\\n\\n# The new Standalone question #\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":{\"__typename\":\"CodeBlockMetadata\",\"mode\":\"EXPLICIT\",\"lang\":\"plaintext\"},\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_34\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_34\",\"name\":\"60c6\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"ROUTER_DECISION_PROMPT\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"CODE\",\"start\":0,\"end\":22,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_35\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_35\",\"name\":\"1e4a\",\"type\":\"PRE\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"# Context #\\nThis is part of a conversational AI system that determines whether to use a retrieval-augmented generator (RAG) or a chat model to answer user questions. \\n\\n#########\\n\\n# Objective #\\nEvaluate the given question and decide whether the RAG application is required to provide a comprehensive answer by retrieving relevant information from a knowledge base, or if the chat model's inherent knowledge is sufficient to generate an appropriate response.\\n\\n#########\\n\\n# Style #\\nThe response should be a clear and direct decision, stated concisely.\\n\\n#########\\n\\n# Tone #\\nAnalytical and objective.\\n\\n#########\\n\\n# Audience #\\nThe audience is the internal system components that will act on the decision.\\n\\n#########\\n\\n# Response #\\nIf the question should be rephrased return response in YAML file format:\\n```\\n    result: true\\n```\\notherwise return in YAML file format:\\n```\\n    result: false\\n```\\n\\n##################\\n\\n# Chat History #\\n{chat_history}\\n\\n#########\\n\\n# User question #\\n{ question }\\n\\n#########\\n\\n# Your Decision in YAML format #\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":{\"__typename\":\"CodeBlockMetadata\",\"mode\":\"EXPLICIT\",\"lang\":\"plaintext\"},\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_36\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_36\",\"name\":\"8774\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"With the prompts defined using the COSTAR framework, we can proceed to implement the _call method and complete the ConversationalRagChain. This method will encapsulate the entire conversation flow, leveraging the prompts and decision-making processes we've established.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"CODE\",\"start\":85,\"end\":90,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null},{\"__typename\":\"Markup\",\"type\":\"CODE\",\"start\":115,\"end\":137,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_37\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_37\",\"name\":\"355b\",\"type\":\"PRE\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"def _call(self, inputs: Dict[str, Any], run_manager: Optional[CallbackManagerForChainRun] = None) -\\u003E Dict[str, Any]:\\n    \\\"\\\"\\\"Call the chain.\\\"\\\"\\\"\\n    chat_history = inputs[self.chat_history_key]\\n    question = inputs[self.input_key]\\n\\n    # chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.\\n    if not chat_history:\\n        answer = self.rag_chain.invoke({\\\"query\\\": question})['result'].strip()\\n        return {self.output_key: answer}\\n\\n    # first check for question rephrasing and then check for standalone question\\n    needs_rephrasing = self.rephrasing_chain.invoke({\\\"chat_history\\\": chat_history, \\\"question\\\": question})['text'].strip()\\n    \\n    # parse the llm response using the output parser\\n    rephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)\\n\\n    # If the question need rephrasing, return the original question\\n    if rephrasing_decision.result:\\n        # Combine previous chat history and question into a single prompt to generate a standalone question.\\n        standalone_question = self.standalone_question_chain.invoke({\\\"chat_history\\\": chat_history, \\\"question\\\": question})['text'].strip()\\n\\n    # check if there are relevant sources in the vectorstore\\n    docs = self.rag_chain._get_docs(question)\\n    if docs:\\n        # use the RAG model\\n        answer = self.rag_chain.invoke({\\\"query\\\": question})['result'].strip()\\n    else:\\n        # send the question to inner router decision, Inner routing --\\u003E use chat model or RAG model\\n        routing_decision = self.router_decision_chain.invoke({\\\"chat_history\\\": chat_history, \\\"question\\\": question},)[\\\"text\\\"].strip()\\n        \\n        # parse the llm response using the output parser\\n        routing_decision = self.yaml_output_parser.parse(routing_decision)\\n        \\n        if routing_decision.result:\\n            # Use chat model\\n            answer = self.llm.invoke(chat_history + [{\\\"role\\\": \\\"user\\\", \\\"content\\\": question}],).content\\n        else:\\n            # get the response from the RAG chain\\n            answer = self.rag_chain.invoke({\\\"query\\\": question})['result'].strip()\\n\\n    return {self.output_key: answer}\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":{\"__typename\":\"CodeBlockMetadata\",\"mode\":\"EXPLICIT\",\"lang\":\"python\"},\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_38\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_38\",\"name\":\"70a3\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"After defining the prompts and implementing the _call method, we can construct the ConversationalRagChain class:\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"CODE\",\"start\":48,\"end\":53,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null},{\"__typename\":\"Markup\",\"type\":\"CODE\",\"start\":83,\"end\":105,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_39\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_39\",\"name\":\"851b\",\"type\":\"PRE\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"class ConversationalRagChain(Chain):\\n    \\\"\\\"\\\"Chain that encpsulate RAG application enablingnatural conversations\\\"\\\"\\\"\\n    rag_chain: Chain\\n    rephrasing_chain: LLMChain\\n    standalone_question_chain: LLMChain\\n    router_decision_chain: LLMChain\\n    yaml_output_parser: YamlOutputParser\\n    \\n    # input\\\\output parameters\\n    input_key: str = \\\"query\\\"  \\n    chat_history_key: str = \\\"chat_history\\\" \\n    output_key: str = \\\"result\\\"\\n\\n    @property\\n    def input_keys(self) -\\u003E List[str]:\\n        \\\"\\\"\\\"Input keys.\\\"\\\"\\\"\\n        return [self.input_key, self.chat_history_key]\\n\\n    @property\\n    def output_keys(self) -\\u003E List[str]:\\n        \\\"\\\"\\\"Output keys.\\\"\\\"\\\"\\n        return [self.output_key]\\n\\n    @property\\n    def _chain_type(self) -\\u003E str:\\n        \\\"\\\"\\\"Return the chain type.\\\"\\\"\\\"\\n        return \\\"ConversationalRagChain\\\"\\n\\n    @classmethod\\n    def from_llm(\\n            cls,\\n            rag_chain: Chain,\\n            llm: BaseLanguageModel,\\n            callbacks: Callbacks = None,\\n            **kwargs: Any,\\n    ) -\\u003E ConversationalRagChain:\\n        \\\"\\\"\\\"Initialize from LLM.\\\"\\\"\\\"\\n    \\n        # create the rephrasing chain\\n        rephrasing_chain = LLMChain(llm=llm, prompt=REPHRASING_PROMPT, callbacks=callbacks)\\n    \\n        # create the standalone question chain\\n        standalone_question_chain = LLMChain(llm=llm, prompt=STANDALONE_PROMPT, callbacks=callbacks)\\n    \\n        # router decision chain\\n        router_decision_chain = LLMChain(llm=llm, prompt=ROUTER_DECISION_PROMPT, callbacks=callbacks)\\n    \\n        return cls(\\n            rag_chain=rag_chain,\\n            rephrasing_chain=rephrasing_chain,\\n            standalone_question_chain=standalone_question_chain,\\n            router_decision_chain=router_decision_chain,\\n            yaml_output_parser=YamlOutputParser(pydantic_object=ResultYAML),\\n            callbacks=callbacks,\\n            **kwargs,\\n        )\\n\\n    def _call(self, inputs: Dict[str, Any], run_manager: Optional[CallbackManagerForChainRun] = None) -\\u003E Dict[str, Any]:\\n        \\\"\\\"\\\"Call the chain.\\\"\\\"\\\"\\n        chat_history = inputs[self.chat_history_key]\\n        question = inputs[self.input_key]\\n    \\n        # chat history is empty, so don't rephrase the question and start a new conversation and direct to RAG chain.\\n        if not chat_history:\\n            answer = self.rag_chain.invoke({\\\"query\\\": question})['result'].strip()\\n            return {self.output_key: answer}\\n    \\n        # first check for question rephrasing and then check for standalone question\\n        needs_rephrasing = self.rephrasing_chain.invoke({\\\"chat_history\\\": chat_history, \\\"question\\\": question})['text'].strip()\\n        \\n        # parse the llm response using the output parser\\n        rephrasing_decision = self.yaml_output_parser.parse(needs_rephrasing)\\n    \\n        # If the question need rephrasing, return the original question\\n        if rephrasing_decision.result:\\n            # Combine previous chat history and question into a single prompt to generate a standalone question.\\n            standalone_question = self.standalone_question_chain.invoke({\\\"chat_history\\\": chat_history, \\\"question\\\": question})['text'].strip()\\n    \\n        # check if there are relevant sources in the vectorstore\\n        docs = self.rag_chain._get_docs(question)\\n        if docs:\\n            # use the RAG model\\n            answer = self.rag_chain.invoke({\\\"query\\\": question})['result'].strip()\\n        else:\\n            # send the question to inner router decision, Inner routing --\\u003E use chat model or RAG model\\n            routing_decision = self.router_decision_chain.invoke({\\\"chat_history\\\": chat_history, \\\"question\\\": question},)[\\\"text\\\"].strip()\\n            \\n            # parse the llm response using the output parser\\n            routing_decision = self.yaml_output_parser.parse(routing_decision)\\n            \\n            if routing_decision.result:\\n                # Use chat model\\n                answer = self.llm.invoke(chat_history + [{\\\"role\\\": \\\"user\\\", \\\"content\\\": question}],).content\\n            else:\\n                # get the response from the RAG chain\\n                answer = self.rag_chain.invoke({\\\"query\\\": question})['result'].strip()\\n    \\n        return {self.output_key: answer}\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":{\"__typename\":\"CodeBlockMetadata\",\"mode\":\"EXPLICIT\",\"lang\":\"python\"},\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_40\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_40\",\"name\":\"cf3b\",\"type\":\"H4\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Final Words\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_41\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_41\",\"name\":\"5619\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"This blog post demonstrated a simple approach to transform a RAG model into a conversational AI tool using LangChain. We designed a conversational flow to determine when to leverage the RAG application or chat model, utilizing the COSTAR framework to craft effective prompts.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:415c67eeaf84_42\":{\"__typename\":\"Paragraph\",\"id\":\"415c67eeaf84_42\",\"name\":\"46f4\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Stay tuned for more insightful posts in my \\\"Mastering RAG Chatbots\\\" series, where I am exploring advanced conversational AI techniques and real-world applications.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"STRONG\",\"start\":44,\"end\":66,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"PostViewerEdge:postId:d740493ff328-viewerId:lo_6efb08673bd8\":{\"__typename\":\"PostViewerEdge\",\"shouldIndexPostForExternalSearch\":true,\"id\":\"postId:d740493ff328-viewerId:lo_6efb08673bd8\"},\"Tag:genai\":{\"__typename\":\"Tag\",\"id\":\"genai\",\"displayTitle\":\"Genai\",\"normalizedTagSlug\":\"genai\"},\"Tag:langchain\":{\"__typename\":\"Tag\",\"id\":\"langchain\",\"displayTitle\":\"Langchain\",\"normalizedTagSlug\":\"langchain\"},\"Tag:conversational-ai\":{\"__typename\":\"Tag\",\"id\":\"conversational-ai\",\"displayTitle\":\"Conversational AI\",\"normalizedTagSlug\":\"conversational-ai\"},\"Tag:retrieval-augmented\":{\"__typename\":\"Tag\",\"id\":\"retrieval-augmented\",\"displayTitle\":\"Retrieval Augmented\",\"normalizedTagSlug\":\"retrieval-augmented\"},\"Tag:llm\":{\"__typename\":\"Tag\",\"id\":\"llm\",\"displayTitle\":\"Llm\",\"normalizedTagSlug\":\"llm\"},\"Membership:6d695e0ea6f8\":{\"__typename\":\"Membership\",\"tier\":\"MEMBER\",\"id\":\"6d695e0ea6f8\"},\"Post:d740493ff328\":{\"__typename\":\"Post\",\"id\":\"d740493ff328\",\"collection\":null,\"content({\\\"postMeteringOptions\\\":{}})\":{\"__typename\":\"PostContent\",\"isLockedPreviewOnly\":false,\"bodyModel\":{\"__typename\":\"RichText\",\"sections\":[{\"__typename\":\"Section\",\"name\":\"f1eb\",\"startIndex\":0,\"textLayout\":null,\"imageLayout\":null,\"backgroundImage\":null,\"videoLayout\":null,\"backgroundVideo\":null}],\"paragraphs\":[{\"__ref\":\"Paragraph:415c67eeaf84_0\"},{\"__ref\":\"Paragraph:415c67eeaf84_1\"},{\"__ref\":\"Paragraph:415c67eeaf84_2\"},{\"__ref\":\"Paragraph:415c67eeaf84_3\"},{\"__ref\":\"Paragraph:415c67eeaf84_4\"},{\"__ref\":\"Paragraph:415c67eeaf84_5\"},{\"__ref\":\"Paragraph:415c67eeaf84_6\"},{\"__ref\":\"Paragraph:415c67eeaf84_7\"},{\"__ref\":\"Paragraph:415c67eeaf84_8\"},{\"__ref\":\"Paragraph:415c67eeaf84_9\"},{\"__ref\":\"Paragraph:415c67eeaf84_10\"},{\"__ref\":\"Paragraph:415c67eeaf84_11\"},{\"__ref\":\"Paragraph:415c67eeaf84_12\"},{\"__ref\":\"Paragraph:415c67eeaf84_13\"},{\"__ref\":\"Paragraph:415c67eeaf84_14\"},{\"__ref\":\"Paragraph:415c67eeaf84_15\"},{\"__ref\":\"Paragraph:415c67eeaf84_16\"},{\"__ref\":\"Paragraph:415c67eeaf84_17\"},{\"__ref\":\"Paragraph:415c67eeaf84_18\"},{\"__ref\":\"Paragraph:415c67eeaf84_19\"},{\"__ref\":\"Paragraph:415c67eeaf84_20\"},{\"__ref\":\"Paragraph:415c67eeaf84_21\"},{\"__ref\":\"Paragraph:415c67eeaf84_22\"},{\"__ref\":\"Paragraph:415c67eeaf84_23\"},{\"__ref\":\"Paragraph:415c67eeaf84_24\"},{\"__ref\":\"Paragraph:415c67eeaf84_25\"},{\"__ref\":\"Paragraph:415c67eeaf84_26\"},{\"__ref\":\"Paragraph:415c67eeaf84_27\"},{\"__ref\":\"Paragraph:415c67eeaf84_28\"},{\"__ref\":\"Paragraph:415c67eeaf84_29\"},{\"__ref\":\"Paragraph:415c67eeaf84_30\"},{\"__ref\":\"Paragraph:415c67eeaf84_31\"},{\"__ref\":\"Paragraph:415c67eeaf84_32\"},{\"__ref\":\"Paragraph:415c67eeaf84_33\"},{\"__ref\":\"Paragraph:415c67eeaf84_34\"},{\"__ref\":\"Paragraph:415c67eeaf84_35\"},{\"__ref\":\"Paragraph:415c67eeaf84_36\"},{\"__ref\":\"Paragraph:415c67eeaf84_37\"},{\"__ref\":\"Paragraph:415c67eeaf84_38\"},{\"__ref\":\"Paragraph:415c67eeaf84_39\"},{\"__ref\":\"Paragraph:415c67eeaf84_40\"},{\"__ref\":\"Paragraph:415c67eeaf84_41\"},{\"__ref\":\"Paragraph:415c67eeaf84_42\"}]},\"validatedShareKey\":\"\",\"shareKeyCreator\":null},\"creator\":{\"__ref\":\"User:55ec5990cf02\"},\"inResponseToEntityType\":null,\"isLocked\":false,\"isMarkedPaywallOnly\":false,\"lockedSource\":\"LOCKED_POST_SOURCE_NONE\",\"mediumUrl\":\"https:\\u002F\\u002Ftalwaitzenberg.com\\u002Fmastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328\",\"primaryTopic\":null,\"topics\":[{\"__typename\":\"Topic\",\"slug\":\"artificial-intelligence\"},{\"__typename\":\"Topic\",\"slug\":\"machine-learning\"},{\"__typename\":\"Topic\",\"slug\":\"programming\"}],\"isPublished\":true,\"latestPublishedVersion\":\"415c67eeaf84\",\"visibility\":\"PUBLIC\",\"postResponses\":{\"__typename\":\"PostResponses\",\"count\":0},\"clapCount\":185,\"allowResponses\":true,\"isLimitedState\":false,\"title\":\"Mastering RAG Chatbots: Building Advanced RAG as a Conversational AI Tool with LangChain\",\"isSeries\":false,\"sequence\":null,\"uniqueSlug\":\"mastering-rag-chatbots-building-advanced-rag-as-a-conversational-ai-tool-with-langchain-d740493ff328\",\"socialTitle\":\"\",\"socialDek\":\"\",\"canonicalUrl\":\"\",\"metaDescription\":\"\",\"latestPublishedAt\":1710196127597,\"readingTime\":8.696540880503145,\"previewContent\":{\"__typename\":\"PreviewContent\",\"subtitle\":\"\"},\"previewImage\":{\"__ref\":\"ImageMetadata:1*Ok5uPTnHHr8WB09oNHl1tQ.png\"},\"isShortform\":false,\"seoTitle\":\"\",\"firstPublishedAt\":1710195293786,\"updatedAt\":1710196131054,\"shortformType\":\"SHORTFORM_TYPE_LINK\",\"seoDescription\":\"\",\"viewerEdge\":{\"__ref\":\"PostViewerEdge:postId:d740493ff328-viewerId:lo_6efb08673bd8\"},\"isSuspended\":false,\"license\":\"ALL_RIGHTS_RESERVED\",\"tags\":[{\"__ref\":\"Tag:genai\"},{\"__ref\":\"Tag:langchain\"},{\"__ref\":\"Tag:conversational-ai\"},{\"__ref\":\"Tag:retrieval-augmented\"},{\"__ref\":\"Tag:llm\"}],\"isNewsletter\":false,\"statusForCollection\":null,\"pendingCollection\":null,\"detectedLanguage\":\"en\",\"wordCount\":2203,\"layerCake\":0}} script: window.main(); script: (function(){function c(){var b=a.contentDocument||a.contentWindow.document;if(b){var d=b.createElement('script');d.innerHTML=\"window.__CF$cv$params={r:'8b8f1d528cde5a12',t:'MTcyNDYyNTk0OS4wMDAwMDA='};var a=document.createElement('script');a.nonce='';a.src='/cdn-cgi/challenge-platform/scripts/jsd/main.js';document.getElementsByTagName('head')[0].appendChild(a);\";b.getElementsByTagName('head')[0].appendChild(d)}}if(document.body){var a=document.createElement('iframe');a.height=1;a.width=1;a.style.position='absolute';a.style.top=0;a.style.left=0;a.style.border='none';a.style.visibility='hidden';document.body.appendChild(a);if('loading'!==document.readyState)c();else if(window.addEventListener)document.addEventListener('DOMContentLoaded',c);else{var e=document.onreadystatechange||function(){};document.onreadystatechange=function(b){e(b);'loading'!==document.readyState&&(document.onreadystatechange=e,c())}}}})(); "
}